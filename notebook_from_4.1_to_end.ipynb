{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YZ9hEhaKv5D"
      },
      "source": [
        "### Cella usa e getta (ogni volta che re-inizializzi il kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_D9MJ9FKv5E",
        "outputId": "89ae455e-b54d-4db7-bd0a-c4b0edc2ae84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]\n",
            "TensorFlow version: 2.12.0\n",
            "keras version = 2.10.0\n"
          ]
        }
      ],
      "source": [
        "# libraries\n",
        "import os\n",
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "if in_colab:\n",
        "    if not os.getcwd().split('/')[-1].split('_')[-1]=='2023':\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        os.chdir(r'/content/drive/MyDrive/Human_Data_Analytics_Project_2023')\n",
        "\n",
        "    if not 'tensorflow_io' in sys.modules:\n",
        "        print('Installing tensorflow-IO')\n",
        "        !pip install tensorflow-io\n",
        "    if not 'keras' in sys.modules:\n",
        "        print('Installing keras')\n",
        "        !pip install keras==2.10.0\n",
        "    if not 'scikeras' in sys.modules:\n",
        "        print('Installing scikeras')\n",
        "        !pip install scikeras[tensorflow]\n",
        "    if not 'keras-tuner' in sys.modules:\n",
        "        print('installing keras tuner')\n",
        "        !pip install keras-tuner\n",
        "        !pip install numba==0.57.0\n",
        "\n",
        "main_dir = os.getcwd()\n",
        "if main_dir not in sys.path:\n",
        "    print('Adding the folder for the modules')\n",
        "    sys.path.append(main_dir)\n",
        "\n",
        "#BASE LIBRARIES\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import shutil\n",
        "import time\n",
        "import random\n",
        "import subprocess\n",
        "import itertools\n",
        "import warnings\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "#PLOT LIBRARIES\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sb\n",
        "sb.set(style=\"white\", palette=\"muted\")\n",
        "import IPython.display as ipd\n",
        "#import plotly.express as px\n",
        "\n",
        "#AUDIO LIBRARIES\n",
        "import librosa\n",
        "from scipy.io import wavfile\n",
        "from scipy import signal\n",
        "from scipy.fft import fft,ifft,fftfreq, fftshift\n",
        "from scipy.signal import stft,spectrogram,periodogram\n",
        "#from pydub import AudioSegment\n",
        "\n",
        "#MACHINE LEARNING LIBRARIES\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, LeaveOneOut, train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.utils import check_random_state\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "import tensorflow as tf\n",
        "\n",
        "# GPU SETTINGS FOR LINUX and repressing warnings for windows. References for gpu: https://www.tensorflow.org/guide/gpu \n",
        "show_gpu_activity = False \n",
        "if sys.platform == 'linux':\n",
        "    if show_gpu_activity:\n",
        "        tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "    # Restrict TensorFlow to only allocate a part of memory on the first GPU\n",
        "        try:\n",
        "            tf.config.set_logical_device_configuration(\n",
        "                gpus[0],\n",
        "                [tf.config.LogicalDeviceConfiguration(memory_limit=6800)])\n",
        "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "        except RuntimeError as e:\n",
        "            # Virtual devices must be set before GPUs have been initialized\n",
        "            print(e)\n",
        "else:\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.utils import plot_model as tf_plot\n",
        "if in_colab:\n",
        "    import tensorflow_io as tfio\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "# show keras version\n",
        "import keras\n",
        "print(f'keras version = {keras.__version__}')\n",
        "#import keras_tune as kt\n",
        "from keras import layers\n",
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.regularizers import L1L2\n",
        "# kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4) # we may use this in some layers...\n",
        "\n",
        "#RANDOM SETTINGS\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "check_random_state(seed)\n",
        "\n",
        "#EVALUATION LIBRAIRES\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import RocCurveDisplay, precision_recall_curve, PrecisionRecallDisplay\n",
        "from sklearn.metrics import precision_recall_fscore_support, auc\n",
        "\n",
        "#OUR PERSONAL FUNCTIONS\n",
        "import importlib\n",
        "from Preprocessing.data_loader import download_dataset,load_metadata\n",
        "from Preprocessing.exploration_plots import one_random_audio, plot_clip_overview, Spectral_Analysis\n",
        "from Models.basic_ml import basic_ML_experiments, basic_ML_experiments_gridsearch, build_dataset, extract_flatten_MFCC\n",
        "from Visualization.model_plot import confusion_matrix,listen_to_wrong_audio\n",
        "\n",
        "importlib.reload(importlib.import_module('Preprocessing.data_loader'))\n",
        "importlib.reload(importlib.import_module('Models.basic_ml'))\n",
        "importlib.reload(importlib.import_module('Visualization.model_plot'))\n",
        "\n",
        "from Preprocessing.data_loader import load_metadata\n",
        "df_ESC10, df_ESC50 = load_metadata(main_dir,heads = False, ESC_US = False, statistics=False)\n",
        "\n",
        "from Preprocessing.data_loader import load_metadata\n",
        "from Models.basic_ml import basic_ML_experiments, basic_ML_experiments_gridsearch, build_dataset, extract_flatten_MFCC\n",
        "\n",
        "importlib.reload(importlib.import_module('Models.ann_utils'))\n",
        "importlib.reload(importlib.import_module('Visualization.model_plot'))\n",
        "\n",
        "from Models.ann_utils import *\n",
        "from Models.ann_utils import MFCCWithDeltaLayer,OutputCutterLayer\n",
        "from Visualization.model_plot import plot_history, confusion_matrix, listen_to_wrong_audio, visualize_the_weights\n",
        "\n",
        "ESC10_path = os.path.join(main_dir,'data', 'ESC-10-depth')\n",
        "samplerate = 44100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wir8MvVHtdh"
      },
      "source": [
        "# 4 ENCODER FOR HIGH-LEVEL FEATURE EXTRACTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Classification on encoded raw audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since neither the basic machine learning appraches nor the dense feed forward NNs were able to achieve a good accuracy, we decided only the RNN approach, which at least achived alone 28% of accurcy on the 10 class classification problem. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the encoder and buil the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"Encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 32, 64, 4)         40        \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 16, 32, 4)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 16, 32, 4)         0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 16, 8)          296       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 4, 8, 8)          0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 4, 8, 8)           0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 2, 4, 12)          876       \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 1, 2, 12)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 1, 2, 12)          0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 24)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 50        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,262\n",
            "Trainable params: 1,262\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "AE_name = 'AE_Conv_prep_flatten_STFT'\n",
        "path_to_AE = os.path.join(main_dir, 'Saved_Models',AE_name)\n",
        "\n",
        "#import the autencoder\n",
        "autoencoder = load_model(path_to_AE)\n",
        "encoder = autoencoder.layers[1]\n",
        "encoder.summary()\n",
        "# freeze the encoder\n",
        "encoder.trainable = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(\n",
        "    encoder_layer = encoder,\n",
        "    n_labels = n_labels,\n",
        "    INPUT_DIM = INPUT_DIM,\n",
        "    n_units = 8,\n",
        "    activation = 'tanh',\n",
        "    #arguments to compile the model\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    learning_rate = 1e-3,\n",
        "    metrics = ['accuracy'],\n",
        "    verbose = 0,\n",
        "    compile = True\n",
        "):\n",
        "\n",
        "    classifier = tf.keras.Sequential([\n",
        "        tf.keras.layers.GRU(units=n_units, activation=activation),\n",
        "        tf.keras.layers.Dense(n_labels, activation='softmax')\n",
        "    ])\n",
        "    if compile:\n",
        "        # compile the model\n",
        "        model.compile(loss = loss,\n",
        "                    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate) if sys.platform == 'darwin' or in_colab else tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                    metrics = metrics)\n",
        "        if verbose > 0:\n",
        "            model.summary()\n",
        "\n",
        "    print(f'filters {n_filters}, kernel size {kernel_size}, units of GRU {n_units}, lr {learning_rate}, activation {activation}, strides {strides}')\n",
        "    print(f' Trainable parameters: {model.count_params()}')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a function that takes the loaded dataset and the encoder and encodes the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_dataset(dataset, encoder, dataset_type = 'tf'):  \n",
        "    # dataset = dataset to encode which can be a tf.data.Dataset or a numpy array\n",
        "    # in the case of a tf dataset the dataset contains both examples and labels; in the case of a numpy array the dataset contains only examples\n",
        "    # type = 'tf' or 'np' if you want to encode the dataset in tensorflow or numpy\n",
        "    # encoder = encoder to use to encode the dataset \n",
        "    # ENCODED_INPUT_DIM = dimension of the encoded dataset\n",
        "    # note: we still have to test the funciton on a image dataset\n",
        "\n",
        "    if dataset_type == 'tf':\n",
        "        dataset = dataset.map(lambda x, lab: (encoder(x), lab))\n",
        "        ENCODED_INPUT_DIM = (dataset.element_spec[0].shape[1],)\n",
        "    if dataset_type == 'np':\n",
        "        dataset = encoder(dataset).numpy()\n",
        "        ENCODED_INPUT_DIM = (dataset.shape[1],)\n",
        "    return dataset, ENCODED_INPUT_DIM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From Basic Machine Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading and encoding the dataset in numpy array format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "build_ESC_US_df = False\n",
        "\n",
        "X_ESC10, y_ESC10, labels10 = build_dataset(df_ESC10)\n",
        "X_ESC50, y_ESC50, labels50 = build_dataset(df_ESC50)\n",
        "df_subset, X_subset, y_subset, labels_subset = build_dataset(df_ESC50, subset = True, num_classes=10)\n",
        "\n",
        "# encoding all the dataset just created\n",
        "X_ESC10_encoded, ENCODED_INPUT_DIM = encode_dataset(X_ESC10, encoder, dataset_type = 'np')\n",
        "X_ESC50_encoded, _ = encode_dataset(X_ESC50, encoder, dataset_type = 'np')\n",
        "X_subset_encoded, _ = encode_dataset(X_subset, encoder, dataset_type = 'np')\n",
        "\n",
        "if build_ESC_US_df:\n",
        "    try:\n",
        "        df_ESC_US\n",
        "    except NameError: df_ESC_US = None\n",
        "\n",
        "    if df_ESC_US is None:\n",
        "        df_ESC_US = load_metadata(main_dir, heads = False, ESC10 = False, ESC50 = False, ESC_US = True)\n",
        "\n",
        "    X_ESC_US = build_dataset(df_ESC_US)\n",
        "    X_ESC_US_encoded, _ = encode_dataset(X_ESC_US, encoder, dataset_type = 'np')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "basic_ML_experiments_gridsearch(X_ESC10_encoded,y_ESC10, test_size = 0.25,\n",
        "                                file = r'Models\\grid_search_results_encoded.txt',# where to save the results\n",
        "                                cv=3,\n",
        "                                verbose = False,\n",
        "                                logistic_regression = False,\n",
        "                                SVM = False,\n",
        "                                decision_tree = False,\n",
        "                                random_forest = False,\n",
        "                                KNN = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_ESC10 = basic_ML_experiments(X_ESC10_encoded, y_ESC10, test_size = 0.25,\n",
        "                                     raw_audio  =True,\n",
        "                                     logistic_regression = True,\n",
        "                                     SVM = True,\n",
        "                                     decision_tree = True,\n",
        "                                     random_forest = True,\n",
        "                                     KNN = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_subset = basic_ML_experiments_autoencoder(X_subset_encoded, y_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_ESC50 = basic_ML_experiments(X_ESC50_encoded, y_ESC50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A seguire i risultati sono riportati per ogni metodo. DA TOGLIERE MFCC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_ESC50 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fully connected from 2.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the full labelled dataset without splitting it into train, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "preprocessing = None\n",
        "n_dim = 1\n",
        "\n",
        "dataset, label = create_dataset_lite(df_ESC50,\n",
        "                                         batch_size = batch_size,\n",
        "                                         preprocessing = preprocessing,\n",
        "                                         ndim = n_dim )\n",
        "\n",
        "INPUT_DIM, n_labels = example_batch(dataset, label_names = list(label.columns),verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Encode the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoded_dataset, ENCODED_INPUT_DIM = encode_dataset(dataset, encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the building function for models for the k fold search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model( n_labels=n_labels,   # arguments to build the model\n",
        "                 INPUT_DIM= ENCODED_INPUT_DIM,\n",
        "                 n_units = 64,\n",
        "                 activation = 'relu',\n",
        "                 #arguments to compile the model\n",
        "                 loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 learning_rate = 1e-3,\n",
        "                 metrics = ['accuracy']):\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=ENCODED_INPUT_DIM),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(n_units, activation=activation),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(n_labels, activation='softmax')\n",
        "        ], name = 'ESC50_encoded_classification')\n",
        "\n",
        "    model.compile(loss = loss,\n",
        "                  optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate) if sys.platform == 'darwin' or in_colab else tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  metrics = metrics)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing the k fold search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "patience = 10\n",
        "params = {'n_units':[64, 128], 'activation':['relu','tanh'], 'learning_rate':[1e-2,1e-3, 1e-4]}\n",
        "K_fold = 5\n",
        "\n",
        "model_cv, result, best_params = K_fold_training(encoded_dataset, build_model, params = params, epochs =  epochs, patience = patience, verbose = 0, K=K_fold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GRU From 2.4.1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "\n",
        "dataset, label = create_dataset_lite(df_ESC50,\n",
        "                                     batch_size = batch_size,\n",
        "                                     preprocessing = None,\n",
        "                                     ndim = 2)\n",
        "INPUT_DIM, n_labels = example_batch(dataset, label_names = list(label.columns), verbose = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoded_dataset, ENCODED_INPUT_DIM = encode_dataset(dataset, encoder)\n",
        "\n",
        "#transform ENCODED_INPUT_DIM from (code_size,) to TensorShape([code_size, 1])\n",
        "ENCODED_INPUT_DIM = tf.TensorShape([ENCODED_INPUT_DIM[0], 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%capture captured_output\n",
        "# https://pypi.org/project/silence-tensorflow/       altra speranza per mutare il log di tensorflow\n",
        "\n",
        "\n",
        "def build_model( n_labels = n_labels,   # arguments to build the model\n",
        "                 INPUT_DIM = ENCODED_INPUT_DIM,\n",
        "                 n_filters = 1,\n",
        "                 n_units = 8,\n",
        "                 kernel_size = 18,\n",
        "                 activation = 'tanh',\n",
        "                 #arguments to compile the model\n",
        "                 loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 learning_rate = 1e-3,\n",
        "                 metrics = ['accuracy'],\n",
        "                 verbose = 0):\n",
        "    strides = int(kernel_size/2)\n",
        "    model = tf.keras.Sequential([\n",
        "        #reduce the dimensionality of the input with a 1DConvolution\n",
        "        tf.keras.layers.Conv1D(filters=n_filters, kernel_size = kernel_size, strides=strides, activation=activation, input_shape=ENCODED_INPUT_DIM),\n",
        "        #with more than 1 filters I'll have prpoblems with the channel dimension not accepted by Recurrent layers\n",
        "        #apply a SimpleRNN layer\n",
        "        tf.keras.layers.GRU(units=n_units, activation=activation),\n",
        "        tf.keras.layers.Dense(n_labels, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # compile the model\n",
        "    model.compile(loss = loss,\n",
        "                  optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate) if sys.platform == 'darwin' or in_colab else tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  metrics = metrics)\n",
        "    if verbose > 0:\n",
        "        model.summary()\n",
        "\n",
        "    print(f'filters {n_filters}, kernel size {kernel_size}, units of GRU {n_units}, lr {learning_rate}, activation {activation}, strides {strides}')\n",
        "    print(f' Trainable parameters: {model.count_params()}')\n",
        "    return model\n",
        "\n",
        "epochs = 100\n",
        "patience = 10\n",
        "\n",
        "params = {'n_filters':[1],'kernel_size':[4,8,16], 'learning_rate':[1e-3, 1e-4], 'n_units':[4,8,16],'activation':['tanh']}\n",
        "K_fold = 5\n",
        "model_cv, result, best_params = K_fold_training(encoded_dataset, build_model, params = params, epochs = epochs, patience = patience, verbose = 0, K=K_fold)\n",
        "\n",
        "\n",
        "#save captured_output.stdout to a file\n",
        "\n",
        "#with open(\"log_gru_encoded.txt\", \"w\") as file:\n",
        "    #file.write(captured_output.stdout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tensorial (Change name) latent space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "#import the encoder in subfolder \"Leo\" named working_encoder.h5\n",
        "encoder = load_model('Leo/working_encoder_tensor.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Try the encoder on a random generated sample\n",
        "\n",
        "a = encoder(np.random.uniform(1, -1, size=(1, 64, 128, 1)))\n",
        "a.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From basic machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "dovrei riscrivere le funzioni di basic machine learning in modo che ci sia la possibilitÃ  di scegliere se usare MFCC, MEL, STFT nel caso del capitolo 2. Inoltre serve anche che si possa fare basic ml anche su flatten code (simile al raw audio) o su un tensore (simile a qualcosa di preprocessed come MFCC, ...) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From CNN 2.3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "importlib.reload(importlib.import_module('Models.ann_utils'))\n",
        "from Models.ann_utils import create_dataset, compile_fit_evaluate, K_fold_training, create_dataset_lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 25\n",
        "\n",
        "dataset, label = create_dataset_lite(df_ESC50,\n",
        "                                     batch_size = batch_size,\n",
        "                                     preprocessing = 'STFT',\n",
        "                                     ndim = 3)\n",
        "INPUT_DIM, n_labels = example_batch(dataset, label_names = list(label.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def resize_images(image, new_height = 64, new_width = 128):\n",
        "    # INPUT: the audio preprocessed by STFT, MEL or MFCC\n",
        "    # OUTPUT: the audio reshaped as a tensor (needed for the convolutional layers)\n",
        "    image = tf.image.resize(image, [new_height, new_width])\n",
        "    return image\n",
        "\n",
        "dataset = dataset.map(lambda audio, label: (tf.py_function(func=resize_images, inp=[audio], Tout=tf.float32), label),\n",
        "                            num_parallel_calls=tf.data.AUTOTUNE,\n",
        "                            deterministic=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_max_lazy(dataset, verbose = 1):\n",
        "    max = 0\n",
        "    lazy_number = 4\n",
        "    for elem, label in dataset.take(lazy_number):\n",
        "        new = tf.reduce_max(tf.abs(elem)) \n",
        "        if new > max:\n",
        "            max = new\n",
        "    if verbose > 0:\n",
        "        print(f'The max value is {max}')\n",
        "    return max.numpy(), elem.numpy()\n",
        "\n",
        "max, _ = find_max_lazy(dataset, verbose = 1)\n",
        "\n",
        "dataset = dataset.map(lambda audio, label: (audio/max, label), num_parallel_calls=tf.data.AUTOTUNE, deterministic = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_dataset(dataset, encoder, dataset_type = 'tf'):  \n",
        "    # dataset = dataset to encode which can be a tf.data.Dataset or a numpy array\n",
        "    # in the case of a tf dataset the dataset contains both examples and labels; in the case of a numpy array the dataset contains only examples\n",
        "    # type = 'tf' or 'np' if you want to encode the dataset in tensorflow or numpy\n",
        "    # encoder = encoder to use to encode the dataset \n",
        "    # ENCODED_INPUT_DIM = dimension of the encoded dataset\n",
        "    # note: we still have to test the funciton on a image dataset\n",
        "\n",
        "    if dataset_type == 'tf':\n",
        "        dataset = dataset.map(lambda x, lab: (encoder(x), lab))\n",
        "        for example, _ in dataset.take(1):\n",
        "            ENCODED_INPUT_DIM = example.shape\n",
        "\n",
        "    if dataset_type == 'np':\n",
        "        dataset = encoder(dataset).numpy()\n",
        "        ENCODED_INPUT_DIM = (dataset.shape[1],)\n",
        "    return dataset, ENCODED_INPUT_DIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset, ENCODED_INPUT_DIM = encode_dataset(dataset, encoder, dataset_type = 'tf')\n",
        "print(ENCODED_INPUT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i,j in dataset.take(1):\n",
        "    print(i.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENCODED_INPUT_DIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_units = 4\n",
        "kernel_size = (2,2,2)\n",
        "activation = 'relu'\n",
        "INPUT_DIM = ENCODED_INPUT_DIM + (1,)\n",
        "l = tf.keras.layers.Conv3D(n_units, kernel_size, strides = 2 ,activation=activation, input_shape=INPUT_DIM, padding=\"same\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_DIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(ENCODED_INPUT_DIM)\n",
        "# add a dimension to ENCODED_INPUT_DIM\n",
        "ENCODED_INPUT_DIM_2 = ENCODED_INPUT_DIM + (1,)\n",
        "print(ENCODED_INPUT_DIM_2)\n",
        "\n",
        "r = np.random.uniform(-1,1,ENCODED_INPUT_DIM_2)\n",
        "print(r.shape)\n",
        "l(r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model( n_labels = n_labels,   # arguments to build the model\n",
        "                 INPUT_DIM = ENCODED_INPUT_DIM,\n",
        "                 n_units = 8,\n",
        "                 kernel_size = (3,3,3),\n",
        "                 activation = 'relu',\n",
        "                 #arguments to compile the model\n",
        "                 loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 learning_rate = 1e-3,\n",
        "                 metrics = ['accuracy'],\n",
        "                 verbose = 0):\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "\n",
        "        tf.keras.layers.InputLayer(input_shape=INPUT_DIM),\n",
        "\n",
        "        # Convolutional layer 1\n",
        "        tf.keras.layers.Conv3D(n_units, kernel_size, strides = 2 ,activation=activation, padding=\"same\"),\n",
        "        tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2), padding = 'same'),\n",
        "\n",
        "        # Convolutional layer 2\n",
        "        tf.keras.layers.Conv3D(2*n_units, (3, 3, 3), strides = 2 ,  activation=activation, padding=\"same\"),\n",
        "        tf.keras.layers.MaxPooling3D(pool_size=(2, 2, 2), padding = 'same'),\n",
        "\n",
        "        # Flatten the output of the previous layer\n",
        "        tf.keras.layers.Flatten(),\n",
        "\n",
        "        # Dense layer for classification\n",
        "        tf.keras.layers.Dense(n_labels, activation='softmax')  # Assuming 10 classes for classification\n",
        "    ], name = 'CNN')\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(loss = loss,\n",
        "                  optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate) if sys.platform == 'darwin' or in_colab else tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  metrics = metrics)\n",
        "    if verbose > 0 :\n",
        "        model.summary()\n",
        "\n",
        "    print(f'n_units {n_units}, activation {activation}, learning_rate {learning_rate}, kernel size {kernel_size}')\n",
        "\n",
        "    return model\n",
        "\n",
        "epochs = 5\n",
        "patience = 20\n",
        "params = {'n_units':[4], 'activation':['relu','tanh'], 'learning_rate':[1e-3, 1e-4], 'kernel_size':[(3,3,3), (2,2,2)]}\n",
        "K_fold = 5\n",
        "\n",
        "model_cv, result, best_params = K_fold_training(dataset, build_model, params = params, epochs =  epochs, patience = patience, verbose = 1, K=K_fold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#refit only the best model\n",
        "#best_params = {'activation': 'relu', 'kernel_size': (7, 7), 'learning_rate': 0.001, 'n_units': 64}\t# best parameters from the grid search\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "ESC10_path = os.path.join(main_dir,'data', 'ESC-10-depth')\n",
        "batch_size = 30\n",
        "preprocessing = 'STFT'\n",
        "\n",
        "train, val, test, label_names, INPUT_DIM, n_labels = create_dataset( ESC10_path,\n",
        "                                                verbose = 1,\n",
        "                                                batch_size = batch_size,\n",
        "                                                validation_split = 0.25, # this is the splitting of train vs validation + test\n",
        "                                                normalize = True, # normalization preprocessing (default is true)\n",
        "                                                preprocessing = preprocessing,   # \"STFT\", \"MEL\", \"MFCC\" or None\n",
        "                                                show_example_batch = True,\n",
        "                                                ndim=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#build the model\n",
        "unit = best_params['n_units']\n",
        "activation_fc = best_params['activation']\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Convolutional layer 1\n",
        "    tf.keras.layers.Conv2D(unit, best_params['kernel_size'], strides = 2 ,activation=activation_fc, input_shape=INPUT_DIM, padding=\"same\"),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Convolutional layer 2\n",
        "    tf.keras.layers.Conv2D(2*unit, (3, 3), strides = 2 ,  activation=activation_fc, padding=\"same\"),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Convolutional layer 3\n",
        "    tf.keras.layers.Conv2D(4*unit, (3, 3), activation=activation_fc, padding=\"same\"),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Flatten the output of the previous layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    # Dense layer for classification\n",
        "    tf.keras.layers.Dense(n_labels, activation='softmax')  # Assuming 10 classes for classification\n",
        "], name = 'first_CNN')\n",
        "\n",
        "epochs = 100\n",
        "patience = 20 # early stopping patience\n",
        "lr = best_params['learning_rate']\n",
        "model, hisotry, confusion_mtx, evaluation = compile_fit_evaluate(df_ESC10, model, train, val, test, label_names,\n",
        "                                     epochs = epochs,\n",
        "                                     patience = patience,\n",
        "                                     loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                                     optimizer =tf.keras.optimizers.Adam(learning_rate=lr) if sys.platform == 'darwin' or in_colab else tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "                                     metrics = ['accuracy'],#,'CategoricalAccuracy'],\n",
        "                                     verbose = 1,\n",
        "                                     show_history = True,\n",
        "                                     show_test_evaluation = True,\n",
        "                                     show_confusion_matrix = True,\n",
        "                                     listen_to_wrong = True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Second Try"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "importlib.reload(importlib.import_module('Models.ann_utils'))\n",
        "from Models.ann_utils import create_dataset, compile_fit_evaluate, K_fold_training, create_dataset_lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 25\n",
        "\n",
        "dataset, label = create_dataset_lite(df_ESC50,\n",
        "                                     batch_size = batch_size,\n",
        "                                     preprocessing = 'STFT',\n",
        "                                     ndim = 3)\n",
        "INPUT_DIM, n_labels = example_batch(dataset, label_names = list(label.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def resize_images(image, new_height = 64, new_width = 128):\n",
        "    # INPUT: the audio preprocessed by STFT, MEL or MFCC\n",
        "    # OUTPUT: the audio reshaped as a tensor (needed for the convolutional layers)\n",
        "    image = tf.image.resize(image, [new_height, new_width])\n",
        "    return image\n",
        "\n",
        "dataset = dataset.map(lambda audio, label: (tf.py_function(func=resize_images, inp=[audio], Tout=tf.float32), label),\n",
        "                            num_parallel_calls=tf.data.AUTOTUNE,\n",
        "                            deterministic=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_max_lazy(dataset, verbose = 1):\n",
        "    max = 0\n",
        "    lazy_number = 4\n",
        "    for elem, label in dataset.take(lazy_number):\n",
        "        new = tf.reduce_max(tf.abs(elem)) \n",
        "        if new > max:\n",
        "            max = new\n",
        "    if verbose > 0:\n",
        "        print(f'The max value is {max}')\n",
        "    return max.numpy(), elem.numpy()\n",
        "\n",
        "max, _ = find_max_lazy(dataset, verbose = 1)\n",
        "\n",
        "dataset = dataset.map(lambda audio, label: (audio/max, label), num_parallel_calls=tf.data.AUTOTUNE, deterministic = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_dataset(dataset, encoder, dataset_type = 'tf'):  \n",
        "    # dataset = dataset to encode which can be a tf.data.Dataset or a numpy array\n",
        "    # in the case of a tf dataset the dataset contains both examples and labels; in the case of a numpy array the dataset contains only examples\n",
        "    # type = 'tf' or 'np' if you want to encode the dataset in tensorflow or numpy\n",
        "    # encoder = encoder to use to encode the dataset \n",
        "    # ENCODED_INPUT_DIM = dimension of the encoded dataset\n",
        "    # note: we still have to test the funciton on a image dataset\n",
        "\n",
        "    if dataset_type == 'tf':\n",
        "        dataset = dataset.map(lambda x, lab: (encoder(x), lab))\n",
        "        for example, _ in dataset.take(1):\n",
        "            ENCODED_INPUT_DIM = example.shape\n",
        "\n",
        "    if dataset_type == 'np':\n",
        "        dataset = encoder(dataset).numpy()\n",
        "        ENCODED_INPUT_DIM = (dataset.shape[1],)\n",
        "    return dataset, ENCODED_INPUT_DIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset, ENCODED_INPUT_DIM = encode_dataset(dataset, encoder, dataset_type = 'tf')\n",
        "print(ENCODED_INPUT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i,j in dataset.take(1):\n",
        "    print(i.shape)\n",
        "    print(j.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the CNN model\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "    \n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    \n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    \n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    \n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "# Define input shape and number of classes\n",
        "input_shape = (10, 20, 32)\n",
        "num_classes = 50\n",
        "\n",
        "# Create the CNN model\n",
        "model = create_cnn_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = dataset.unbatch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "k = 0\n",
        "data_list = []\n",
        "\n",
        "for example, label in dataset:\n",
        "    data_list.append([example.numpy().reshape((4, 3, 2)), np.argmax(label.numpy())])\n",
        "    k += 1\n",
        "\n",
        "data_array = np.array(data_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#first element of df\n",
        "data_array[0][0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "labels = data_array[:, 1]\n",
        "data = data_array[:, 0]\n",
        "print(labels)\n",
        "\n",
        "# Preprocess the dataset and labels\n",
        "X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train_encoded = to_categorical(y_train, num_classes)\n",
        "y_val_encoded = to_categorical(y_val, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, y_train_encoded, epochs=10, validation_data=(X_val, y_val_encoded))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "collapsed_sections": [
        "e1m8-j-OhZFk",
        "KZKl4jfLiDm6",
        "cQuihGLn1PiQ",
        "uEfj8ZTMKv5X",
        "knjA2SwyzLtU",
        "4tU8KtanKv5a",
        "F0yn3mv5sp_5",
        "RZ3RcP6Usp__",
        "S6ZoHKYTzLtf"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
