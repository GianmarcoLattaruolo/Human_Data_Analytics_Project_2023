#import all the required libraries
import numpy as np
import pandas as pd
import librosa
import IPython.display as ipd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sb
import os
import sys
import shutil
import random
import time
import importlib
importlib.reload(importlib.import_module('Visualization.model_plot'))
from Visualization.model_plot import plot_history, confusion_matrix, listen_to_wrong_audio
from tensorflow.keras.models import save_model



class ModelSaveCallback(tf.keras.callbacks.Callback):

    def __init__(self, file_name):
        super(ModelSaveCallback, self).__init__()
        self.file_name = file_name

    def on_epoch_end(self, epoch, logs=None):
        save_model(self.model, self.file_name)
        print(f"Epoch {epoch} - Model saved in {self.file_name}")


@tf.autograph.experimental.do_not_convert
def create_dataset(subfolder_path, # folder of the audio data we want to import
                   verbose = 1, # 0, 1 level of verbose
                   batch_size = 30,  
                   shuffle = True, 
                   validation_split = 0.25, # this is the splitting of train vs validation + test
                   cache_file_train = None, # str path of the chaching file 
                   cache_file_val = None, 
                   cache_file_test = None, 
                   normalize = True, # normalization preprocessing 
                   num_repeat = None, # number of times the dataset is repeated
                   preprocessing = None,  # "STFT", "MEL", "MFCC"
                   delta = True,  # True or False only if preprocessing = "MFCC"
                   delta_delta = True,  #  True or False only if preprocessing = "MFCC"
                   labels = "inferred", # labels = 'inferred' or None (for unsupervised learning)
                   resize = False, # we can resize the images generated by the preprocessing
                   new_height = 64,
                   new_width = 128,
                   save_entire = None, # save the entire dataset as a tfrecord file
                   save_train = None, # save the dataset as a tfrecord file
                   save_val = None,
                   save_test = None,
                   show_example_batch = False, # show an example of the batch
                   ): 
                   # INPUT: str - path of the audio directory 
                   # OUTPUT: train, validation, test set as tensorflow dataset

    sample_rate = 44100
    if verbose > 0:
        print("Creating dataset from folder 3: ", subfolder_path)

    #check if the dataset are already saved 
    if save_train is not None and save_test is not None and save_val is not None:
        if os.path.exists(save_train) and os.path.exists(save_test) and os.path.exists(save_val):
            if verbose > 0:
                print("Loading the dataset from the saved file")
            train = tf.data.Dataset.load(save_train)
            val = tf.data.Dataset.load(save_val)
            test = tf.data.Dataset.load(save_test)

            if labels is not None:
                print(f"Warning, if you need the labels you can"+"'"+f"t load the dataset from the saved file, delete one of the {save_train}, {save_test}, {save_val}")
            return train, val, test
    
    #auxiliary functions

    @tf.autograph.experimental.do_not_convert
    def squeeze(audio, labels=None):
        # INPUT: audio as a tf dataset
        # OUTPUT: audio + labels or only audio
        # used to remove a dimension from the tensor
        if audio.shape[-1] is None:
            audio = tf.squeeze(audio, axis=-1) 
        if labels is not None:
            return audio, labels
        else:
            return audio
  
    @tf.autograph.experimental.do_not_convert
    def spectral_preprocessing_audio(audio, 
                                     sample_rate = 44100, 
                                     segment = 20, 
                                     n_fft = None, #padd the frames with zeros before DFT
                                     overlapping = 10, 
                                     cepstral_num = 40, 
                                     N_filters = 50, 
                                     preprocessing = preprocessing,
                                     delta = delta, 
                                     delta_delta = delta_delta):
                                     # INPUT: audio as a tensorflow object
                                     # OUTPUT: preprocessed audio with STFT, MEL, MFCC as desired 
                                     # used to perform the spectral preprocessing (STFT, MEL, MFCC with/out delta, delta-delta)

        

                            
        audio = audio.numpy()

        # transform the segment and overlapping from ms to samples
        if n_fft is None:
            n_fft = segment
        nperseg = round(sample_rate * segment / 1000)
        noverlap = round(sample_rate * overlapping / 1000)
        n_fft = round(sample_rate * n_fft / 1000)
        hop_length = nperseg - noverlap
        r = None

        # using librosa to perform the preprocessing
        if preprocessing == "STFT":
            stft_librosa = librosa.stft(audio, hop_length=hop_length, win_length=nperseg, n_fft=n_fft)
            r = librosa.amplitude_to_db(np.abs(stft_librosa), ref=np.max)

        elif preprocessing == "MEL":
            mel_y = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=n_fft, hop_length=hop_length,
                                                   win_length=nperseg) 
            r = librosa.power_to_db(mel_y, ref=np.max)
        elif preprocessing == "MFCC":
            mfcc_y = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=cepstral_num, n_fft=n_fft,
                                          hop_length=hop_length, htk=True, fmin=40, n_mels=N_filters)

            # now we calculate the delta and delta-delta if needed
            if delta:
                delta_mfccs = librosa.feature.delta(mfcc_y)
                if delta_delta:
                    delta2_mfccs = librosa.feature.delta(mfcc_y, order=2)
                if delta and not delta_delta:
                    mfccs_features = np.concatenate((mfcc_y, delta_mfccs))
                elif delta and delta_delta:
                    mfccs_features = np.concatenate((mfcc_y, delta_mfccs, delta2_mfccs))
            if not delta and not delta_delta:
                mfccs_features = mfcc_y
            r = mfccs_features

        return r
    
    @tf.autograph.experimental.do_not_convert
    def reshape_tensor(matrix):
        # INPUT: the audio preprocessed by STFT, MEL or MFCC
        # OUTPUT: the audio reshaped as a tensor (needed for the convolutional layers)
        matrix = tf.squeeze(matrix)
        tensor = tf.expand_dims(matrix, axis=2)
        return tensor
    
    # @tf.autograph.experimental.do_not_convert
    def find_max_lazy(train):
        max = 0
        lazy_number = 0
        if labels:
            for batch, label in train:
                if lazy_number<4:
                    new = tf.reduce_max(tf.abs(batch)) 
                    if new > max:
                        max = new
                lazy_number+=1
            if verbose > 0:
                print(f'The max value is {max}')
        else:
            for batch in train:
                if lazy_number<4:
                    new = tf.reduce_max(tf.abs(batch)) 
                    if new > max:
                        max = new
                lazy_number+=1
            if verbose > 0:
                print(f'The max value is {max}')            
        
        return max
    
    @tf.autograph.experimental.do_not_convert
    def reshape_images_map(image, new_height = new_height, new_width = new_width):
        # INPUT: the audio preprocessed by STFT, MEL or MFCC
        # OUTPUT: the audio reshaped as a tensor (needed for the convolutional layers)
        image = tf.image.resize(image, [new_height, new_width])
        return image

    @tf.autograph.experimental.do_not_convert
    def decode_ogg(ogg_path):
        # INPUT: the path of the audio file
        # OUTPUT: the audio as a numpy array
        ogg_path = ogg_path.numpy().decode("utf-8")
        ogg_audio = librosa.load(ogg_path, sr = sample_rate)
        return ogg_audio

    # creation of the tf dataset from an audio folder 
    if labels is not None:

        train, val_test = tf.keras.utils.audio_dataset_from_directory(
            directory =subfolder_path.replace('\\','/'),
            labels=labels, 
            label_mode='categorical',
            class_names=None,
            batch_size=None,
            sampling_rate=None,
            output_sequence_length=220500,
            ragged=False,
            shuffle=shuffle,
            seed=42,
            validation_split=validation_split,
            subset='both',
        )

        if labels is not None:
            label_names = np.array(train.class_names)
        if verbose > 0:
            print("label names:", label_names)
              
        # dropping the extra dimension from the tensors 
        train = train.map(squeeze, tf.data.AUTOTUNE)
        val_test = val_test.map(squeeze, tf.data.AUTOTUNE)

    else:
        if verbose>0:
            print('You are using an unlabelled dataset')
        dataset = tf.data.Dataset.list_files(subfolder_path + '/*.ogg', shuffle=False)
        #dataset = dataset.map(lambda path: tf.py_function(func=decode_ogg, inp=[path], Tout=tf.float32))
        dataset = dataset.inteleave(lambda path:  tf.py_function(func=decode_ogg, inp=[path], Tout=tf.float32), block_length=16, num_parallel_calls=tf.data.AUTOTUNE )

        # Get the total number of samples in the dataset
        dataset_size = dataset.cardinality().numpy()
        if verbose > 0:
            print(f"Dataset size: {dataset_size} samples")

        # Calculate the number of samples for validation
        validation_size = int(dataset_size * validation_split)
        if verbose > 0:
            print(f"Validation size: {validation_size} samples")

        # Split the dataset into train and validation sets
        train = dataset.skip(validation_size)
        val_test = dataset.take(validation_size)    


    # Split the validation and test set (val and test set always have the same cardinality)
    val_size = round(val_test.cardinality().numpy() * (1 - validation_split))
    test_size = val_test.cardinality().numpy() - val_size
    test = val_test.shard(num_shards=2, index=0)
    val = val_test.shard(num_shards=2, index=1)

    #now we actually map the raw data to the preprocessed data 
    
    if preprocessing:
        # we need to separate the cases of labelled and unlabelled since the map function works on the whole dataset
        # and not only on some columns 
        if labels:
            train = train.map(lambda audio, target: (tf.py_function(spectral_preprocessing_audio,
                                                                [audio],
                                                                [tf.float32]), target))
            train = train.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
            val = val.map(lambda audio, target: (tf.py_function(spectral_preprocessing_audio,   
                                                                [audio],
                                                                [tf.float32]), target))
            val = val.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
            test = test.map(lambda audio, target: (tf.py_function(spectral_preprocessing_audio,
                                                                [audio],
                                                                [tf.float32]), target))
            test = test.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
        else:
            train = train.map(lambda audio: tf.py_function(spectral_preprocessing_audio, [audio], [tf.float32]),
                            tf.data.AUTOTUNE)
            train = train.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)
            val = val.map(lambda audio: tf.py_function(spectral_preprocessing_audio, [audio], [tf.float32]),
                            tf.data.AUTOTUNE)
            val = val.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)
            test = test.map(lambda audio: tf.py_function(spectral_preprocessing_audio, [audio], [tf.float32]),
                            tf.data.AUTOTUNE)
            test = test.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)

        if resize:
            if labels:
                train = train.map(lambda image, target: (tf.py_function(reshape_images_map, [image],[tf.float32]), target))
                train = train.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
                val = val.map(lambda image, target: (tf.py_function(reshape_images_map, [image], [tf.float32]), target))
                val = val.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
                test = test.map(lambda image, target: (tf.py_function(reshape_images_map, [image], [tf.float32]), target))
                test = test.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
            else:
                train = train.map(lambda image: tf.py_function(reshape_images_map, [image], [tf.float32]))
                train = train.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)
                val = val.map(lambda image: tf.py_function(reshape_images_map, [image], [tf.float32]))
                val = val.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)
                test = test.map(lambda image: tf.py_function(reshape_images_map, [image], [tf.float32]))
                test = test.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)


    if resize and not preprocessing:
        print("You can't resize the images if you don't preprocess them first")
            
    # Normalization step
    if normalize:
        max = find_max_lazy(train)

        if labels:
            train = train.map(lambda matrix, target: (matrix/max, target), tf.data.AUTOTUNE)
            val = val.map(lambda matrix, target: (matrix/max, target), tf.data.AUTOTUNE)
            test = test.map(lambda matrix, target: (matrix/max, target), tf.data.AUTOTUNE)
        else:
            train = train.map(lambda matrix : matrix/max, tf.data.AUTOTUNE)
            val = val.map(lambda matrix : matrix/max, tf.data.AUTOTUNE)
            test = test.map(lambda matrix : matrix/max, tf.data.AUTOTUNE)


    if labels is None:
        # Duplicate data for the autoencoder (input = output)
        train = train.map(lambda x: (x, x))
        val = val.map(lambda x: (x, x))
        test = test.map(lambda x: (x, x))


    # Caching the dataset
    def cache_dataset(dataset, cache_file):
        if cache_file:
            cache_dir = os.path.dirname(cache_file)
            os.makedirs(cache_dir, exist_ok=True)
            dataset = dataset.cache(cache_file)
            if verbose > 0:
                print(f"Cached in file {cache_file}")
        return dataset

    if verbose > 0 and cache_file_train is not None:
        print("Caching the dataset")
    train = cache_dataset(train, cache_file_train)
    val = cache_dataset(val, cache_file_val)
    test = cache_dataset(test, cache_file_test)

    # Shuffling the dataset 
    if shuffle:
        train = train.shuffle(train.cardinality().numpy(), reshuffle_each_iteration=True)
        # We should not shuffle the validation and test set
        #val = val.shuffle(val_size, reshuffle_each_iteration=True)
        #test = test.shuffle(test_size, reshuffle_each_iteration=True)
        # Batching the dataset 
        
    if batch_size:
        train = train.batch(batch_size)
        val = val.batch(batch_size)
        test = test.batch(batch_size)

    # Used in order to pad the dataset
    AUTOTUNE = tf.data.AUTOTUNE
    if num_repeat is not None:
        train = train.repeat(num_repeat).prefetch(buffer_size=AUTOTUNE)
        val = val.repeat(num_repeat).prefetch(buffer_size=AUTOTUNE)
        test = test.repeat(num_repeat).prefetch(buffer_size=AUTOTUNE)
    else:
        train = train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
        val = val.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
        test = test.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

    #saving the dataset as save
    def save_dataset(dataset, save_file):
        if save_file:
            save_dir = os.path.dirname(save_file)
            #check if the directory exists
            if not os.path.exists(save_dir):
                os.mkdir(save_dir)
            else:
                #check if the file exists
                if os.path.exists(save_file):
                    shutil.rmtree(save_file)

            dataset.save(save_file)
            if verbose>0:
                print(f"Dataset saved in {save_file}")
        return dataset
    
    if verbose>0 and save_train is not None:
        print("Saving the datasets...")
    train = save_dataset(train, save_train)
    val = save_dataset(val, save_val)
    test = save_dataset(test, save_test)


    if labels is not None:
        if show_example_batch:
            if verbose > 0:
                print("Showing example batch")
            INPUT_DIM, num_classes = example_batch(train, val, test, label_names, verbose=verbose)
            return train, val, test, label_names, INPUT_DIM, num_classes
        else:
            return train, val, test, label_names
    else:
        if show_example_batch:
            INPUT_DIM = example_batch(train, val, test, verbose)
            return train, val, test, INPUT_DIM
        else:
            return train, val, test



def example_batch(train, val, test, label_names=None, 
                  verbose=1,
                  show_figure = True,
                  check_val_test = False,
                  listen_to_audio = True,):
    if verbose == 0:
        show_figure = False
        check_val_test = False
        listen_to_audio = False

    for example_train_batch, label in train.take(1):
        if verbose > 0:
            print(f'Audio shape: {example_train_batch.shape}')
            if label_names is not None:
                print(f'Label shape: {label.shape}')
                print(f'Label: {label_names[np.where(label.numpy()[0]==1)[0][0]]}')
        INPUT_DIM = example_train_batch.shape[1:]
        if len(INPUT_DIM) == 1 and show_figure:
            plt.subplots(1, 1, figsize=(9, 5))
            plt.tight_layout(pad=3)
            plt.plot(example_train_batch[0].numpy())
            if label_names is not None:
                plt.title('Audio wave plot of class: '+label_names[np.where(label.numpy()[0]==1)[0][0]])
            else:
                plt.title('Audio wave plot')
        if len(INPUT_DIM) > 1 and show_figure:
            plt.figure(figsize=(10, 4))
            plt.imshow(example_train_batch[0].numpy(), aspect = 'auto')
            plt.colorbar()
            if label_names is not None:
                plt.title(f'Spectrogram of class {label_names[np.where(label.numpy()[0]==1)[0][0]]}')
            else:
                plt.title('Spectrogram')
        if len(INPUT_DIM) == 1 and listen_to_audio:
            display(ipd.Audio(example_train_batch[0].numpy(), rate=44100))

    if check_val_test:
        for example_val_batch, label in val.take(1):
            print(f'Audio shape in validation : {example_val_batch.shape}')
            print(f'Label shape in validation : {label.shape}')

        for example_test_batch, label in test.take(1):
            print(f'Audio shape in test : {example_test_batch.shape}')
            print(f'Label shape in test : {label.shape}')

    return INPUT_DIM, len(label_names)


def compile_and_fit(model, train_data, val_data, 
                    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), 
                    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), 
                    metrics = ['accuracy'],
                    patience = 5,
                    epochs = 10,
                    steps_per_epoch = None,
                    verbose = 1,
                    model_filename = None):
    
    if verbose > 0:
        model.summary()
    
    model.compile(optimizer=optimizer,
                  loss=loss,
                  metrics=metrics)
    
    if model_filename is not None:
        callbacks = [tf.keras.callbacks.EarlyStopping(verbose=verbose, patience=patience),
                     ModelSaveCallback(model_filename)]
        #cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1)

    else:
        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', 
                                                      mode='max', 
                                                      verbose=verbose,
                                                      restore_best_weights=True, 
                                                      patience=patience)]
    
    history = model.fit(train_data,
                        epochs=epochs,
                        validation_data=val_data,
                        callbacks=callbacks,
                        steps_per_epoch = steps_per_epoch,
                        verbose=verbose)
    return model,history


@tf.autograph.experimental.do_not_convert
def compile_fit_evaluate(data_frame, model, train, val, test, label_names=None,
                         load_model = False,
                         model_path = None,
                         loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), 
                         optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), 
                         metrics = ['accuracy'],
                         patience = 5,
                         epochs = 10,
                         steps_per_epoch = None,
                         verbose = 1,
                         show_history = True,
                         show_test_evaluation = True,
                         show_confusion_matrix = True,
                         listen_to_wrong = True,
                         save_the_model = False,
                         name_model = None,
                         save_weights_only = False,
                         save_format = 'tf',
                         save_best_only = True,
                         model_filename = None,
                         ):
    if load_model:
        if model_path is None:
            print("You need to specify the path to load the model")
        else:
            model = tf.keras.models.load_model(model_path)

    model,history = compile_and_fit(model, train, val,
                                    optimizer=optimizer,
                                    loss=loss,  
                                    metrics=metrics,
                                    patience=patience,
                                    epochs=epochs,
                                    steps_per_epoch=steps_per_epoch,
                                    verbose=verbose,
                                    model_filename=model_filename)

    if show_history:
        plot_history(history)

    if show_test_evaluation:
        scores = model.evaluate(test, return_dict=True)
        display(scores)

    if show_confusion_matrix:
        #predict the test set
        y_pred = model.predict(test)
        y_pred = tf.argmax(y_pred, axis=1)

        y_true = tf.concat(list(test.map(lambda s,lab: lab)), axis=0)
        y_true = tf.argmax(y_true,axis=1)

        #confusion matrix
        confusion_mtx = confusion_matrix(y_true, y_pred, label_names)

    if listen_to_wrong:
        listen_to_wrong_audio(data_frame, y_true, y_pred, label_names, confusion_mtx)

    if save_the_model:
        if name_model is None:
            print("You need to specify the path to save the model")
        else:
            path_to_save = os.path.join(main_dir,'models',name_model)
            if save_weights_only:
                model.save_weights(path_to_save, save_format=save_format)
            else:
                model.save(path_to_save, save_format=save_format, save_best_only=save_best_only)

    if show_confusion_matrix:
        if show_test_evaluation:
            return model, history, confusion_mtx, scores
        
    else:
        return model, history
    


