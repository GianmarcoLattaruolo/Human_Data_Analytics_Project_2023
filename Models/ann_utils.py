#import all the required libraries
import numpy as np
import pandas as pd
import librosa
import IPython.display as ipd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sb
import os
import sys
import random
import time
import importlib
importlib.reload(importlib.import_module('Visualization.model_plot'))
from Visualization.model_plot import plot_history, confusion_matrix, listen_to_wrong_audio

@tf.autograph.experimental.do_not_convert
def create_dataset(subfolder_path, # folder of the audio data we want to import
                   verbose = 1, # 0, 1 level of verbose
                   batch_size = 30,  
                   shuffle = True, 
                   validation_split = 0.25, # this is the splitting of train vs validation + test
                   cache_file_train = None, # str path of the chaching file 
                   cache_file_val = None, 
                   cache_file_test = None, 
                   normalize = True, # normalization preprocessing 
                   num_repeat = None, # number of times the dataset is repeated
                   preprocessing = None,  # "STFT", "MEL", "MFCC"
                   delta = True,  # True or False only if preprocessing = "MFCC"
                   delta_delta = True,  #  True or False only if preprocessing = "MFCC"
                   labels = "inferred", # labels = 'inferred' or None (for unsupervised learning)
                   resize = False, # we can resize the images generated by the preprocessing
                   new_height = 64,
                   new_width = 128): 
                   # INPUT: str - path of the audio directory 
                   # OUTPUT: train, validation, test set as tensorflow dataset

    sample_rate = 44100
    if verbose > 0:
        print("Creating dataset from folder 3: ", subfolder_path)
    
    #auxiliary functions

    @tf.autograph.experimental.do_not_convert
    def squeeze(audio, labels=None):
        # INPUT: audio as a tf dataset
        # OUTPUT: audio + labels or only audio
        # used to remove a dimension from the tensor
        if audio.shape[-1] is None:
            audio = tf.squeeze(audio, axis=-1) 
        if labels is not None:
            return audio, labels
        else:
            return audio
  
    @tf.autograph.experimental.do_not_convert
    def spectral_preprocessing_audio(audio, 
                                     sample_rate = 44100, 
                                     segment = 20, 
                                     n_fft = None, #padd the frames with zeros before DFT
                                     overlapping = 10, 
                                     cepstral_num = 40, 
                                     N_filters = 50, 
                                     preprocessing = preprocessing,
                                     delta = delta, 
                                     delta_delta = delta_delta):
                                     # INPUT: audio as a tensorflow object
                                     # OUTPUT: preprocessed audio with STFT, MEL, MFCC as desired 
                                     # used to perform the spectral preprocessing (STFT, MEL, MFCC with/out delta, delta-delta)

        

                            
        audio = audio.numpy()

        # transform the segment and overlapping from ms to samples
        if n_fft is None:
            n_fft = segment
        nperseg = round(sample_rate * segment / 1000)
        noverlap = round(sample_rate * overlapping / 1000)
        n_fft = round(sample_rate * n_fft / 1000)
        hop_length = nperseg - noverlap
        r = None

        # using librosa to perform the preprocessing
        if preprocessing == "STFT":
            stft_librosa = librosa.stft(audio, hop_length=hop_length, win_length=nperseg, n_fft=n_fft)
            r = librosa.amplitude_to_db(np.abs(stft_librosa), ref=np.max)

        elif preprocessing == "MEL":
            mel_y = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=n_fft, hop_length=hop_length,
                                                   win_length=nperseg) 
            r = librosa.power_to_db(mel_y, ref=np.max)
        elif preprocessing == "MFCC":
            mfcc_y = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=cepstral_num, n_fft=n_fft,
                                          hop_length=hop_length, htk=True, fmin=40, n_mels=N_filters)

            # now we calculate the delta and delta-delta if needed
            if delta:
                delta_mfccs = librosa.feature.delta(mfcc_y)
                if delta_delta:
                    delta2_mfccs = librosa.feature.delta(mfcc_y, order=2)
                if delta and not delta_delta:
                    mfccs_features = np.concatenate((mfcc_y, delta_mfccs))
                elif delta and delta_delta:
                    mfccs_features = np.concatenate((mfcc_y, delta_mfccs, delta2_mfccs))
            if not delta and not delta_delta:
                mfccs_features = mfcc_y
            r = mfccs_features

        return r
    
    @tf.autograph.experimental.do_not_convert
    def reshape_tensor(matrix):
        # INPUT: the audio preprocessed by STFT, MEL or MFCC
        # OUTPUT: the audio reshaped as a tensor (needed for the convolutional layers)
        matrix = tf.squeeze(matrix)
        tensor = tf.expand_dims(matrix, axis=2)
        return tensor
    
    # @tf.autograph.experimental.do_not_convert
    def find_max_lazy(train):
        max = 0
        lazy_number = 0
        if labels:
            for batch, label in train:
                if lazy_number<4:
                    new = tf.reduce_max(tf.abs(batch)) 
                    if new > max:
                        max = new
                lazy_number+=1
            if verbose > 0:
                print(f'The max value is {max}')
        else:
            for batch in train:
                if lazy_number<4:
                    new = tf.reduce_max(tf.abs(batch)) 
                    if new > max:
                        max = new
                lazy_number+=1
            if verbose > 0:
                print(f'The max value is {max}')            
        
        return max
    
    @tf.autograph.experimental.do_not_convert
    def reshape_images_map(image, new_height = new_height, new_width = new_width):
        # INPUT: the audio preprocessed by STFT, MEL or MFCC
        # OUTPUT: the audio reshaped as a tensor (needed for the convolutional layers)
        image = tf.image.resize(image, [new_height, new_width])
        return image

    @tf.autograph.experimental.do_not_convert
    def decode_ogg(ogg_path):
        # INPUT: the path of the audio file
        # OUTPUT: the audio as a numpy array
        ogg_path = ogg_path.numpy().decode("utf-8")
        ogg_audio = librosa.load(ogg_path, sr = sample_rate)
        return ogg_audio

    # creation of the tf dataset from an audio folder 
    if labels is not None:

        train, val_test = tf.keras.utils.audio_dataset_from_directory(
            directory =subfolder_path.replace('\\','/'),
            labels=labels, 
            label_mode='categorical',
            class_names=None,
            batch_size=None,
            sampling_rate=None,
            output_sequence_length=220500,
            ragged=False,
            shuffle=shuffle,
            seed=42,
            validation_split=validation_split,
            subset='both',
        )

        if labels is not None:
            label_names = np.array(train.class_names)
        if verbose > 0:
            print("label names:", label_names)
              
        # dropping the extra dimension from the tensors 
        train = train.map(squeeze, tf.data.AUTOTUNE)
        val_test = val_test.map(squeeze, tf.data.AUTOTUNE)

    else:
        if verbose>0:
            print('You are using an unlabelled dataset')
        dataset = tf.data.Dataset.list_files(subfolder_path + '/*.ogg', shuffle=False)
        dataset = dataset.map(lambda path: tf.py_function(func=decode_ogg, inp=[path], Tout=tf.float32))

        # Get the total number of samples in the dataset
        dataset_size = dataset.cardinality().numpy()

        # Calculate the number of samples for validation
        validation_size = int(dataset_size * validation_split)

        # Split the dataset into train and validation sets
        train = dataset.skip(validation_size)
        val_test = dataset.take(validation_size)    


    # Split the validation and test set (val and test set always have the same cardinality)
    val_size = round(val_test.cardinality().numpy() * (1 - validation_split))
    test_size = val_test.cardinality().numpy() - val_size
    test = val_test.shard(num_shards=2, index=0)
    val = val_test.shard(num_shards=2, index=1)

    #now we actually map the raw data to the preprocessed data 
    
    if preprocessing:
        # we need to separate the cases of labelled and unlabelled since the map function works on the whole dataset
        # and not only on some columns 
        if labels:
            train = train.map(lambda audio, target: (tf.py_function(spectral_preprocessing_audio,
                                                                [audio],
                                                                [tf.float32]), target))
            train = train.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
            val = val.map(lambda audio, target: (tf.py_function(spectral_preprocessing_audio,   
                                                                [audio],
                                                                [tf.float32]), target))
            val = val.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
            test = test.map(lambda audio, target: (tf.py_function(spectral_preprocessing_audio,
                                                                [audio],
                                                                [tf.float32]), target))
            test = test.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
        else:
            train = train.map(lambda audio: tf.py_function(spectral_preprocessing_audio, [audio], [tf.float32]),
                            tf.data.AUTOTUNE)
            train = train.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)
            val = val.map(lambda audio: tf.py_function(spectral_preprocessing_audio, [audio], [tf.float32]),
                            tf.data.AUTOTUNE)
            val = val.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)
            test = test.map(lambda audio: tf.py_function(spectral_preprocessing_audio, [audio], [tf.float32]),
                            tf.data.AUTOTUNE)
            test = test.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)

        if resize:
            if labels:
                train = train.map(lambda image, target: (tf.py_function(reshape_images_map, [image],[tf.float32]), target))
                train = train.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
                val = val.map(lambda image, target: (tf.py_function(reshape_images_map, [image], [tf.float32]), target))
                val = val.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
                test = test.map(lambda image, target: (tf.py_function(reshape_images_map, [image], [tf.float32]), target))
                test = test.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
            else:
                train = train.map(lambda image: tf.py_function(reshape_images_map, [image], [tf.float32]))
                train = train.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)
                val = val.map(lambda image: tf.py_function(reshape_images_map, [image], [tf.float32]))
                val = val.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)
                test = test.map(lambda image: tf.py_function(reshape_images_map, [image], [tf.float32]))
                test = test.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)


    if resize and not preprocessing:
        print("You can't resize the images if you don't preprocess them first")
            
    # Normalization step
    if normalize:
        max = find_max_lazy(train)

        if labels:
            train = train.map(lambda matrix, target: (matrix/max, target), tf.data.AUTOTUNE)
            val = val.map(lambda matrix, target: (matrix/max, target), tf.data.AUTOTUNE)
            test = test.map(lambda matrix, target: (matrix/max, target), tf.data.AUTOTUNE)
        else:
            train = train.map(lambda matrix : matrix/max, tf.data.AUTOTUNE)
            val = val.map(lambda matrix : matrix/max, tf.data.AUTOTUNE)
            test = test.map(lambda matrix : matrix/max, tf.data.AUTOTUNE)

    if labels is None:
        # Duplicate data for the autoencoder (input = output)
        train = train.map(lambda x: (x, x))
        val = val.map(lambda x: (x, x))
        test = test.map(lambda x: (x, x))

    # Caching the dataset 
    if verbose>0:
        print("Caching the dataset")
    if cache_file_train:
        train = train.cache(cache_file_train)
        if verbose>0:
            print("Train cached in file ", cache_file_train)
    if cache_file_val:
        val = val.cache(cache_file_val)
        if verbose>0:
            print("Validation cached in file ", cache_file_val)
    if cache_file_test:
        test = test.cache(cache_file_test)
        if verbose>0:
            print("Test cached in file ", cache_file_test)

    # Shuffling the dataset 
    if shuffle:
        train = train.shuffle(train.cardinality().numpy(), reshuffle_each_iteration=True)
        # We should not shuffle the validation and test set
        #val = val.shuffle(val_size, reshuffle_each_iteration=True)
        #test = test.shuffle(test_size, reshuffle_each_iteration=True)
        # Batching the dataset 
        
    if batch_size:
        train = train.batch(batch_size)
        val = val.batch(batch_size)
        test = test.batch(batch_size)

    # Used in order to pad the dataset
    AUTOTUNE = tf.data.AUTOTUNE
    if num_repeat is not None:
        train = train.repeat(num_repeat).prefetch(buffer_size=AUTOTUNE)
        val = val.repeat(num_repeat).prefetch(buffer_size=AUTOTUNE)
        test = test.repeat(num_repeat).prefetch(buffer_size=AUTOTUNE)
    else:
        train = train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
        val = val.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
        test = test.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)


    if labels is not None:
        return train, val, test, label_names
    else:
        return train, val, test




def compile_and_fit(model, train_data, val_data, 
                    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), 
                    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), 
                    metrics = ['accuracy'],
                    patience = 5,
                    epochs = 10,
                    steps_per_epoch = None,
                    verbose = 1):
    
    if verbose > 0:
        model.summary()
    
    model.compile(optimizer=optimizer,
                  loss=loss,
                  metrics=metrics)
    
    callbacks = tf.keras.callbacks.EarlyStopping(verbose=verbose, patience=patience)
    
    history = model.fit(train_data,
                        epochs=epochs,
                        validation_data=val_data,
                        callbacks=callbacks,
                        steps_per_epoch = steps_per_epoch,
                        verbose=verbose)
    return model,history




@tf.autograph.experimental.do_not_convert
def compile_fit_evaluate(data_frame, model, train, val, test, label_names=None,
                         load_model = False,
                         model_path = None,
                         loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), 
                         optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), 
                         metrics = ['accuracy'],
                         patience = 5,
                         epochs = 10,
                         steps_per_epoch = None,
                         verbose = 1,
                         show_history = True,
                         show_test_evaluation = True,
                         show_confusion_matrix = True,
                         listen_to_wrong = True,
                         save_the_model = False,
                         name_model = None,
                         save_weights_only = False,
                         save_format = 'tf',
                         save_best_only = True,
                         ):
    if load_model:
        if model_path is None:
            print("You need to specify the path to load the model")
        else:
            model = tf.keras.models.load_model(model_path)

    model,history = compile_and_fit(model, train, val,
                                    optimizer=optimizer,
                                    loss=loss,  
                                    metrics=metrics,
                                    patience=patience,
                                    epochs=epochs,
                                    steps_per_epoch=steps_per_epoch,
                                    verbose=verbose)

    if show_history:
        plot_history(history)

    if show_test_evaluation:
        scores = model.evaluate(test, return_dict=True)
        display(scores)

    if show_confusion_matrix:
        #predict the test set
        y_pred = model.predict(test)
        y_pred = tf.argmax(y_pred, axis=1)

        y_true = tf.concat(list(test.map(lambda s,lab: lab)), axis=0)
        y_true = tf.argmax(y_true,axis=1)

        #confusion matrix
        confusion_mtx = confusion_matrix(y_true, y_pred, label_names)

    if listen_to_wrong:
        listen_to_wrong_audio(data_frame, y_true, y_pred, label_names, confusion_mtx)

    if save_the_model:
        if name_model is None:
            print("You need to specify the path to save the model")
        else:
            path_to_save = os.path.join(main_dir,'models',name_model)
            if save_weights_only:
                model.save_weights(path_to_save, save_format=save_format)
            else:
                model.save(path_to_save, save_format=save_format, save_best_only=save_best_only)

    if show_confusion_matrix:
        if show_test_evaluation:
            return model, history, confusion_mtx, scores
        
    else:
        return model, history
    


