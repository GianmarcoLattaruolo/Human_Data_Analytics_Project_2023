#import all the required libraries
import numpy as np
import pandas as pd
import librosa
import IPython.display as ipd
import tensorflow as tf
import matplotlib.pyplot as plt
import os
import sys
import shutil
import random
import time
import importlib
importlib.reload(importlib.import_module('Visualization.model_plot'))
from Visualization.model_plot import plot_history, confusion_matrix, listen_to_wrong_audio
from tensorflow.keras.models import save_model
from scikeras.wrappers import KerasClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve
from sklearn.metrics import make_scorer
from sklearn.metrics import RocCurveDisplay, precision_recall_curve, PrecisionRecallDisplay
from sklearn.metrics import precision_recall_fscore_support, auc
from sklearn.model_selection import GridSearchCV




class ModelSaveCallback(tf.keras.callbacks.Callback):

    def __init__(self, file_name):
        super(ModelSaveCallback, self).__init__()
        self.file_name = file_name

    def on_epoch_end(self, epoch, logs=None):
        save_model(self.model, self.file_name)
        print(f"Epoch {epoch} - Model saved in {self.file_name}")


@tf.autograph.experimental.do_not_convert
def create_dataset(subfolder_path, # folder of the audio data we want to import
                   verbose = 1, # 0, 1 level of verbose
                   batch_size = 30,  
                   shuffle = True, 
                   validation_split = 0.25, # this is the splitting of train vs validation + test
                   cache_file_train = '', # str path of the chaching file or '' to cache in memory
                   cache_file_val = '', 
                   cache_file_test = '', 
                   normalize = True, # normalization preprocessing 
                   num_repeat = None, # number of times the dataset is repeated
                   preprocessing = None,  # "STFT", "MEL", "MFCC"
                   delta = True,  # True or False only if preprocessing = "MFCC"
                   delta_delta = True,  #  True or False only if preprocessing = "MFCC"
                   labels = "inferred", # labels = 'inferred' or None (for unsupervised learning)
                   resize = False, # we can resize the images generated by the preprocessing
                   new_height = 64,
                   new_width = 128,
                   save_train = None, # save the dataset as a tfrecord file
                   save_val = None,
                   save_test = None,
                   save_labels = None, #path to the file without the extension (always txt)
                   show_example_batch = False, # show an example of the batch
                   ndim = 2,
                   transpose = True,
                   ): 
                   # INPUT: str - path of the audio directory 
                   # OUTPUT: train, validation, test set as tensorflow dataset

    save_labels = save_labels + '.txt'

    sample_rate = 44100
    #check if the dataset are already saved and eventually load them
    if labels is not None:
        if save_train is not None and save_test is not None and save_val is not None and save_labels is not None:

            if os.path.exists(save_train) and os.path.exists(save_test) and os.path.exists(save_val) and os.path.exists(save_labels):
                if verbose > 0:
                    print("Loading the dataset from the saved file")
                train = tf.data.Dataset.load(save_train)
                val = tf.data.Dataset.load(save_val)
                test = tf.data.Dataset.load(save_test)
                with open(save_labels, 'r') as f:
                    label_names = [line.rstrip('\n') for line in f]

                if show_example_batch:
                    INPUT_DIM, n_labels = example_batch(train, val, test, label_names, verbose=verbose)
                    return train, val, test, label_names, INPUT_DIM, n_labels
                else:
                    return train, val, test, label_names
                
            #tell if you have just some of train, val and test saved, not all
            elif os.path.exists(save_train) or os.path.exists(save_test) or os.path.exists(save_val) or os.path.exists(save_labels):
                print("You have just some of the files saved, not all of them")
                print("Proceding with normal dataset building.")

    else:
        if save_train is not None and save_test is not None and save_val is not None:
            if os.path.exists(save_train) and os.path.exists(save_test) and os.path.exists(save_val):
                if verbose > 0:
                    print("Loading the dataset from the saved file")
                train = tf.data.Dataset.load(save_train)
                val = tf.data.Dataset.load(save_val)
                test = tf.data.Dataset.load(save_test)

                if show_example_batch:
                    INPUT_DIM = example_batch(train, val, test, verbose=verbose)
                    return train, val, test, INPUT_DIM
                else:
                    return train, val, test
            #tell if you have just some of train, val and test saved, not all
            elif os.path.exists(save_train) or os.path.exists(save_test) or os.path.exists(save_val):
                print("You have just some of the files saved, not all of them")
                print("Proceding with normal dataset building.")

    if verbose > 0:
        print("Creating dataset from folder 3: ", subfolder_path)

    #auxiliary functions
    @tf.autograph.experimental.do_not_convert
    def squeeze(audio, labels=None):
        # INPUT: audio as a tf dataset
        # OUTPUT: audio + labels or only audio
        # used to remove a dimension from the tensor
        if audio.shape[-1] is None:
            audio = tf.squeeze(audio, axis=-1) 
        if labels is not None:
            return audio, labels
        else:
            return audio
  
    @tf.autograph.experimental.do_not_convert
    def spectral_preprocessing_audio(audio, 
                                     sample_rate = 44100, 
                                     segment = 20, 
                                     n_fft = None, #padd the frames with zeros before DFT
                                     overlapping = 10, 
                                     cepstral_num = 40, 
                                     N_filters = 50, 
                                     preprocessing = preprocessing,
                                     delta = delta, 
                                     delta_delta = delta_delta):
                                     # INPUT: audio as a tensorflow object
                                     # OUTPUT: preprocessed audio with STFT, MEL, MFCC as desired 
                                     # used to perform the spectral preprocessing (STFT, MEL, MFCC with/out delta, delta-delta)

        

                            
        audio = audio.numpy()

        # transform the segment and overlapping from ms to samples
        if n_fft is None:
            n_fft = segment
        nperseg = round(sample_rate * segment / 1000)
        noverlap = round(sample_rate * overlapping / 1000)
        n_fft = round(sample_rate * n_fft / 1000)
        hop_length = nperseg - noverlap
        r = None

        # using librosa to perform the preprocessing
        if preprocessing == "STFT":
            stft_librosa = librosa.stft(audio, hop_length=hop_length, win_length=nperseg, n_fft=n_fft)
            r = librosa.amplitude_to_db(np.abs(stft_librosa), ref=np.max)


        elif preprocessing == "MEL":
            mel_y = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=n_fft, hop_length=hop_length,
                                                   win_length=nperseg) 
            r = librosa.power_to_db(mel_y, ref=np.max)
        elif preprocessing == "MFCC":
            mfcc_y = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=cepstral_num, n_fft=n_fft,
                                          hop_length=hop_length, htk=True, fmin=40, n_mels=N_filters)

            # now we calculate the delta and delta-delta if needed
            if delta:
                delta_mfccs = librosa.feature.delta(mfcc_y)
                if delta_delta:
                    delta2_mfccs = librosa.feature.delta(mfcc_y, order=2)
                if delta and not delta_delta:
                    mfccs_features = np.concatenate((mfcc_y, delta_mfccs))
                elif delta and delta_delta:
                    mfccs_features = np.concatenate((mfcc_y, delta_mfccs, delta2_mfccs))
            if not delta and not delta_delta:
                mfccs_features = mfcc_y
            r = mfccs_features

        return r
    
    @tf.autograph.experimental.do_not_convert
    def reshape_tensor(matrix):
        # INPUT: the audio preprocessed by STFT, MEL or MFCC
        # OUTPUT: the audio reshaped as a tensor (needed for the convolutional layers)
        matrix = tf.squeeze(matrix)
        tensor = tf.expand_dims(matrix, axis=2)
        return tensor
    
    # @tf.autograph.experimental.do_not_convert
    def find_max_lazy(train):
        max = 0
        lazy_number = 0
        if labels:
            for batch, label in train:
                if lazy_number<4:
                    new = tf.reduce_max(tf.abs(batch)) 
                    if new > max:
                        max = new
                lazy_number+=1
            if verbose > 0:
                print(f'The max value is {max}')
        else:
            for batch in train:
                if lazy_number<4:
                    new = tf.reduce_max(tf.abs(batch)) 
                    if new > max:
                        max = new
                lazy_number+=1
            if verbose > 0:
                print(f'The max value is {max}')            
        
        return max
    
    @tf.autograph.experimental.do_not_convert
    def reshape_images_map(image, new_height = new_height, new_width = new_width):
        # INPUT: the audio preprocessed by STFT, MEL or MFCC
        # OUTPUT: the audio reshaped as a tensor (needed for the convolutional layers)
        image = tf.image.resize(image, [new_height, new_width])
        return image

    @tf.autograph.experimental.do_not_convert
    def decode_ogg(ogg_path):
        # INPUT: the path of the audio file
        # OUTPUT: the audio as a numpy array
        ogg_path = ogg_path.numpy().decode("utf-8")
        ogg_audio = librosa.load(ogg_path, sr = sample_rate)
        return ogg_audio

    def reshape(audio, ndim = ndim, preprocessing = preprocessing, verbose = verbose, tranpose = transpose):
        if preprocessing is not None:
            if not ndim in [2,3]:
                raise ValueError(f'ndim must be 2 or 3, not {ndim}')
            if ndim == 2:
                audio = tf.squeeze(audio) 
                if transpose:
                    audio = tf.transpose(audio)
            elif ndim == 3:
                audio = tf.squeeze(audio)
                if transpose:
                    audio = tf.transpose(audio)
                audio = tf.expand_dims(audio, axis=2)
        else:
            if not ndim in [1,2]:
                raise ValueError(f'ndim must be 1 or 2, not {ndim}')
            if ndim == 1:
                audio = tf.squeeze(audio)
            elif ndim == 2:
                audio = tf.expand_dims(audio, axis=1)
        return audio
    # creation of the tf dataset from an audio folder 
    if labels is not None:

        train, val_test = tf.keras.utils.audio_dataset_from_directory(
            directory =subfolder_path.replace('\\','/'),
            labels=labels, 
            label_mode='categorical',
            class_names=None,
            batch_size=None,
            sampling_rate=None,
            output_sequence_length=220500,
            ragged=False,
            shuffle=shuffle,
            seed=42,
            validation_split=validation_split,
            subset='both',
        )

        if labels is not None:
            label_names = np.array(train.class_names)
        if verbose > 0:
            print("label names:", label_names)
              
        # dropping the extra dimension from the tensors 
        train = train.map(squeeze, tf.data.AUTOTUNE)
        val_test = val_test.map(squeeze, tf.data.AUTOTUNE)

    else:
        if verbose>0:
            print('You are using an unlabelled dataset')
        dataset = tf.data.Dataset.list_files(subfolder_path + '/*.ogg', shuffle=False)
        dataset = dataset.map(lambda path: tf.py_function(func=decode_ogg, inp=[path], Tout=tf.float32))
        #dataset = dataset.interleave(lambda path:  tf.py_function(func=decode_ogg, inp=[path], Tout=tf.float32), block_length=16, num_parallel_calls=tf.data.AUTOTUNE )

        # Get the total number of samples in the dataset
        dataset_size = dataset.cardinality().numpy()
        if verbose > 0:
            print(f"Dataset size: {dataset_size} samples")

        # Calculate the number of samples for validation
        validation_size = int(dataset_size * validation_split)
        if verbose > 0:
            print(f"Validation size: {validation_size} samples")

        # Split the dataset into train and validation sets
        train = dataset.skip(validation_size)
        val_test = dataset.take(validation_size)    


    # Split the validation and test set (val and test set always have the same cardinality)
    val_size = round(val_test.cardinality().numpy() * (1 - validation_split))
    test_size = val_test.cardinality().numpy() - val_size
    test = val_test.shard(num_shards=2, index=0)
    val = val_test.shard(num_shards=2, index=1)

    #now we actually map the raw data to the preprocessed data 
    
    if preprocessing:
        # we need to separate the cases of labelled and unlabelled since the map function works on the whole dataset
        # and not only on some columns 
        if labels:
            train = train.map(lambda audio, target: (tf.py_function(spectral_preprocessing_audio,
                                                                [audio],
                                                                [tf.float32]), target))
            train = train.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
            val = val.map(lambda audio, target: (tf.py_function(spectral_preprocessing_audio,   
                                                                [audio],
                                                                [tf.float32]), target))
            val = val.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
            test = test.map(lambda audio, target: (tf.py_function(spectral_preprocessing_audio,
                                                                [audio],
                                                                [tf.float32]), target))
            test = test.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
        else:
            train = train.map(lambda audio: tf.py_function(spectral_preprocessing_audio, [audio], [tf.float32]),
                            tf.data.AUTOTUNE)
            train = train.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)
            val = val.map(lambda audio: tf.py_function(spectral_preprocessing_audio, [audio], [tf.float32]),
                            tf.data.AUTOTUNE)
            val = val.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)
            test = test.map(lambda audio: tf.py_function(spectral_preprocessing_audio, [audio], [tf.float32]),
                            tf.data.AUTOTUNE)
            test = test.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)

        if resize:
            if labels:
                train = train.map(lambda image, target: (tf.py_function(reshape_images_map, [image],[tf.float32]), target))
                #train = train.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
                val = val.map(lambda image, target: (tf.py_function(reshape_images_map, [image], [tf.float32]), target))
                #val = val.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
                test = test.map(lambda image, target: (tf.py_function(reshape_images_map, [image], [tf.float32]), target))
                #test = test.map(lambda matrix, target:(reshape_tensor(matrix), target), tf.data.AUTOTUNE)
            else:
                train = train.map(lambda image: tf.py_function(reshape_images_map, [image], [tf.float32]))
                #train = train.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)
                val = val.map(lambda image: tf.py_function(reshape_images_map, [image], [tf.float32]))
                #val = val.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)
                test = test.map(lambda image: tf.py_function(reshape_images_map, [image], [tf.float32]))
                #test = test.map(lambda matrix: reshape_tensor(matrix), tf.data.AUTOTUNE)


    if resize and not preprocessing:
        print("You can't resize the images if you don't preprocess them first")
            
    # Normalization step
    if normalize:
        max = find_max_lazy(train)

        if labels:
            train = train.map(lambda matrix, target: (matrix/max, target), tf.data.AUTOTUNE)
            val = val.map(lambda matrix, target: (matrix/max, target), tf.data.AUTOTUNE)
            test = test.map(lambda matrix, target: (matrix/max, target), tf.data.AUTOTUNE)
        else:
            train = train.map(lambda matrix : matrix/max, tf.data.AUTOTUNE)
            val = val.map(lambda matrix : matrix/max, tf.data.AUTOTUNE)
            test = test.map(lambda matrix : matrix/max, tf.data.AUTOTUNE)


    if labels is None:
        # Duplicate data for the autoencoder (input = output)
        train = train.map(lambda x: (x, x))
        val = val.map(lambda x: (x, x))
        test = test.map(lambda x: (x, x))

    #final check for the dimension with reshape
    train = train.map(lambda audio, target: (reshape(audio), target), tf.data.AUTOTUNE)
    val = val.map(lambda audio, target: (reshape(audio), target), tf.data.AUTOTUNE)
    test = test.map(lambda audio, target: (reshape(audio), target), tf.data.AUTOTUNE)

    # Caching the dataset
    def cache_dataset(dataset, cache_file):
        if cache_file == '':
            dataset = dataset.cache()
        else:    
            cache_dir = os.path.dirname(cache_file)
            os.makedirs(cache_dir, exist_ok=True)
            dataset = dataset.cache(cache_file)
        if verbose > 0:
            if cache_file == '':
                print("Caching the dataset in memory")
            else:
                print(f"Cached in file {cache_file}")
        return dataset

    if verbose > 0 and cache_file_train is not None:
        print("Caching the dataset")
    #train = cache_dataset(train, cache_file_train)
    #val = cache_dataset(val, cache_file_val)
    #test = cache_dataset(test, cache_file_test)

    # Shuffling the dataset 
    if shuffle:
        train = train.shuffle(train.cardinality().numpy(), reshuffle_each_iteration=True)
        # We should not shuffle the validation and test set
        #val = val.shuffle(val_size, reshuffle_each_iteration=True)
        #test = test.shuffle(test_size, reshuffle_each_iteration=True)
        # Batching the dataset 
        
    if batch_size:
        train = train.batch(batch_size)
        val = val.batch(batch_size)
        test = test.batch(batch_size)

    # Used in order to pad the dataset
    AUTOTUNE = tf.data.AUTOTUNE
    if num_repeat is not None:
        train = train.repeat(num_repeat).prefetch(buffer_size=AUTOTUNE)
        val = val.repeat(num_repeat).prefetch(buffer_size=AUTOTUNE)
        test = test.repeat(num_repeat).prefetch(buffer_size=AUTOTUNE)
    else:
        train = train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
        val = val.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
        test = test.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

    #saving the dataset as save
    def save_dataset(dataset, save_file):
        if save_file and type(dataset)!=np.ndarray:
            save_dir = os.path.dirname(save_file)
            #check if the directory exists
            if not os.path.exists(save_dir):
                os.mkdir(save_dir)
            else:
                #check if the file exists
                if os.path.exists(save_file):
                    shutil.rmtree(save_file)

            dataset.save(save_file)
            if verbose>0:
                print(f"Dataset saved in {save_file}")
                
        elif save_file and type(dataset)==np.ndarray:
        
            #check if the file .txt already exists
            if os.path.exists(save_file):
                os.remove(save_file)

            #create an empty txt file
            with open(save_file, 'w') as f:
                for s in dataset:
                    f.write(str(s) + '\n')

        return dataset
    
    if verbose>0 and save_train is not None:
        print("Saving the datasets...")

    #saving the datasets
    train = save_dataset(train, save_train)
    val = save_dataset(val, save_val)
    test = save_dataset(test, save_test)
    #saving the labels
    if labels is not None:
        if verbose > 0:
            print("Saving the labels...")
        label_names = save_dataset(label_names, save_labels)
    


    #Several return statements depending on the needs
    if labels is not None:
        if show_example_batch:
            if verbose > 0:
                print("Showing example batch")
            INPUT_DIM, num_classes = example_batch(train, val, test, label_names, verbose=verbose)
            return train, val, test, label_names, INPUT_DIM, num_classes
        else:
            return train, val, test, label_names
    else:
        if show_example_batch:
            INPUT_DIM = example_batch(train, val, test, verbose)
            return train, val, test, INPUT_DIM
        else:
            return train, val, test

@tf.autograph.experimental.do_not_convert
def create_dataset_lite(data_frame = None,
                        labeled = True,
                        path_to_ogg_files = None,
                        batch_size = 30,
                        preprocessing = None, # "STFT", "MEL", "MFCC" or None,
                        sample_rate = 44100,
                        segment = 20,
                        overlap = 10,
                        cepstral_num = 40,
                        N_filters = 50,
                        ndim = 3, # number of dimension of the single elemnt in the output dataset,
                        transpose = True,
                        delta = True,
                        delta_delta = True,
                        verbose = 1,
                        save_dataset_path = None,  #path to the folder to save the dataset,
                        load_saved_dataset = None, #path to the folder to load the dataset
                        cache_file = '', #path to the folder to cache the dataset or '' to cache in memory
                        resize = False,
                        new_height = 64,
                        new_width = 128):

    # set all seed
    seed = 42
    tf.random.set_seed(seed)
    np.random.seed(seed)

    #AUXILIARY FUNCTIONS
    def load_audio(path):
        # INPUT: the path of the audio file
        # OUTPUT: the audio as a numpy array
        path = path.numpy().decode("utf-8")
        audio = librosa.load(path, sr = sample_rate)
        return audio

    def spectral_preprocessing(preprocessing, 
                            audio, 
                            sample_rate = 44100, 
                            delta = delta, 
                            delta_delta = delta_delta,
                            segment = segment,
                            overlap = overlap,
                            cepstral_num = cepstral_num,
                            N_filters = N_filters):
        
                            
            audio = audio.numpy()

            # transform the segment and overlapping from ms to samples
            nperseg = round(sample_rate * segment / 1000)
            noverlap = round(sample_rate * overlap / 1000)
            n_fft = round(sample_rate * segment / 1000)
            hop_length = nperseg - noverlap


            # using librosa to perform the preprocessing
            if preprocessing == "STFT":
                stft_librosa = librosa.stft(audio, hop_length=hop_length, win_length=nperseg, n_fft=n_fft)
                r = librosa.amplitude_to_db(np.abs(stft_librosa), ref=np.max)

            elif preprocessing == "MEL":
                mel_y = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_fft=n_fft, hop_length=hop_length,
                                                    win_length=nperseg) 
                r = librosa.power_to_db(mel_y, ref=np.max)
            elif preprocessing == "MFCC":
                mfcc_y = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=cepstral_num, n_fft=n_fft,
                                            hop_length=hop_length, htk=True, fmin=40, n_mels=N_filters)

                # now we calculate the delta and delta-delta if needed
                if delta:
                    delta_mfccs = librosa.feature.delta(mfcc_y)
                    if delta_delta:
                        delta2_mfccs = librosa.feature.delta(mfcc_y, order=2)
                    if delta and not delta_delta:
                        mfccs_features = np.concatenate((mfcc_y, delta_mfccs))
                    elif delta and delta_delta:
                        mfccs_features = np.concatenate((mfcc_y, delta_mfccs, delta2_mfccs))
                if not delta and not delta_delta:
                    mfccs_features = mfcc_y
                r = mfccs_features

            return r

    def find_max_lazy(dataset, labels = labeled, verbose = verbose):
            max = 0
            lazy_number = 4
            if labels:
                for audio, label in dataset.take(lazy_number):
                    new = tf.reduce_max(tf.abs(audio)) 
                    if new > max:
                        max = new
                if verbose > 0:
                    print(f'The max value is {max}')
            else:
                for audio in dataset.take(lazy_number):
                    new = tf.reduce_max(tf.abs(audio)) 
                    if new > max:
                        max = new
                if verbose > 0:
                    print(f'The max value is {max}')            
            
            return max, audio.numpy()

    def save_dataset(dataset, save_file):
        if save_file:
            save_dir = os.path.dirname(save_file)
            #check if the directory exists
            if not os.path.exists(save_dir):
                os.mkdir(save_dir)
            else:
                #check if the file exists
                if os.path.exists(save_file):
                    shutil.rmtree(save_file)

            dataset.save(save_file)
            if verbose>0:
                print(f"Dataset saved in {save_file}")
        return dataset

    def laod_dataset(dataset_path):
        dataset = tf.data.Dataset.load(dataset_path)
        return dataset

    def reshape(audio, ndim = ndim, preprocessing = preprocessing, verbose = verbose, tranpose = transpose):
        if preprocessing is not None:
            if not ndim in [2,3]:
                raise ValueError(f'ndim must be 2 or 3, not {ndim}')
            if ndim == 2:
                audio = tf.squeeze(audio) 
                if transpose:
                    audio = tf.transpose(audio)
            elif ndim == 3:
                audio = tf.squeeze(audio)
                if transpose:
                    audio = tf.transpose(audio)
                audio = tf.expand_dims(audio, axis=2)
        else:
            if not ndim in [1,2]:
                raise ValueError(f'ndim must be 1 or 2, not {ndim}')
            if ndim == 1:
                audio = tf.squeeze(audio)
            elif ndim == 2:
                audio = tf.expand_dims(audio, axis=1)
        return audio

    def resize_images(image, new_height = new_height, new_width = new_width):
        # INPUT: the audio preprocessed by STFT, MEL or MFCC
        # OUTPUT: the audio reshaped as a tensor (needed for the convolutional layers)
        image = tf.image.resize(image, [new_height, new_width])
        return image

    #load save dataset if available
    if load_saved_dataset is not None:
        dataset = laod_dataset(load_saved_dataset)
        dataset = dataset.cache(filename = cache_file).shuffle(dataset.cardinality().numpy(), reshuffle_each_iteration=True).batch(batch_size).prefetch(tf.data.AUTOTUNE)
        return dataset

    # dataset creation
    (file_path,labels) = (data_frame.full_path, pd.get_dummies(data_frame.category))
    dataset = tf.data.Dataset.from_tensor_slices((file_path, labels))
    dataset  = dataset.map(lambda path, label: (tf.py_function(func=load_audio, inp=[path], Tout=tf.float32), label))
    
    # spectral preprocessing and resizing
    if preprocessing is not None:
        if verbose > 0:
            print(f'Preprocessing: {preprocessing}')
        if labeled:
            dataset = dataset.map(lambda audio, label: (tf.py_function(func=spectral_preprocessing, inp=[preprocessing,audio], Tout=tf.float32), label),
                            num_parallel_calls=tf.data.AUTOTUNE,
                            deterministic=False)
            if resize:
                if verbose > 0:
                    print(f'Resizing with shape: {new_height}x{new_width}')
                dataset = dataset.map(lambda audio, label: (tf.py_function(func=resize_images, inp=[audio], Tout=tf.float32), label),
                            num_parallel_calls=tf.data.AUTOTUNE,
                            deterministic=False)
        else:
            dataset = dataset.map(lambda audio: tf.py_function(func=spectral_preprocessing, inp=[preprocessing,audio], Tout=tf.float32),
                            num_parallel_calls=tf.data.AUTOTUNE,
                            deterministic=False)
            if resize:
                if verbose > 0:
                    print(f'Resizing with shape: {new_height}x{new_width}')
                dataset = dataset.map(lambda audio: tf.py_function(func=resize_images, inp=[audio], Tout=tf.float32),
                            num_parallel_calls=tf.data.AUTOTUNE,
                            deterministic=False)
        
    #normalization
    max, example_audio = find_max_lazy(dataset)
    if labeled:
        dataset = dataset.map(lambda audio, label: (audio/max, label), num_parallel_calls=tf.data.AUTOTUNE, deterministic = False)
    else:
        dataset = dataset.map(lambda audio: audio/max, num_parallel_calls=tf.data.AUTOTUNE, deterministic = False)    

    #adjust the shape of the elements in the dataset:
    if len(example_audio.shape)!=ndim:
        if labeled:
            dataset = dataset.map(lambda audio, label: (tf.py_function(func = reshape, inp = [audio], Tout = tf.float32), label), 
                            num_parallel_calls=tf.data.AUTOTUNE, 
                            deterministic = False)
        else:
            dataset = dataset.map(lambda audio: tf.py_function(func = reshape, inp = [audio], Tout = tf.float32),
                                num_parallel_calls=tf.data.AUTOTUNE, 
                                deterministic = False)

    # save the dataset in a path
    if save_dataset_path is not None:
        dataset = save_dataset(dataset, save_dataset_path)

    # final rutine
    dataset = dataset.cache(filename = cache_file)
    #dataset = dataset.shuffle(dataset.cardinality().numpy(), reshuffle_each_iteration=True)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    if labeled is not None:
        return dataset, labels


def example_batch(train, val= None, test = None, label_names=None, 
                  verbose=1,
                  show_figure = True,
                  check_val_test = False,
                  listen_to_audio = True,):
    if verbose == 0:
        show_figure = False
        check_val_test = False
        listen_to_audio = False

    for example_train_batch, label in train.take(1):
        if verbose > 0:
            print(f'Audio shape: {example_train_batch.shape}')
            if label_names is not None:
                print(f'Label shape: {label.shape}')
                print(f'Label: {label_names[np.where(label.numpy()[0]==1)[0][0]]}')
        INPUT_DIM = example_train_batch.shape[1:]
        if (len(INPUT_DIM) == 1 or (INPUT_DIM[-1]==1 and len(INPUT_DIM)==2)) and show_figure:
            plt.subplots(1, 1, figsize=(9, 5))
            plt.tight_layout(pad=3)
            plt.plot(example_train_batch[0].numpy())
            if label_names is not None:
                plt.title('Audio wave plot of class: '+label_names[np.where(label.numpy()[0]==1)[0][0]])
            else:
                plt.title('Audio wave plot')
        elif len(INPUT_DIM) > 1 and show_figure:
            plt.figure(figsize=(10, 4))
            plt.imshow(example_train_batch[0].numpy(), aspect = 'auto')
            plt.colorbar()
            if label_names is not None:
                plt.title(f'Spectrogram of class {label_names[np.where(label.numpy()[0]==1)[0][0]]}')
            else:
                plt.title('Spectrogram')

        if len(INPUT_DIM) == 1 and listen_to_audio:
            display(ipd.Audio(example_train_batch[0].numpy(), rate=44100))

    if check_val_test and val is not None and test is not None:
        for example_val_batch, label in val.take(1):
            print(f'Audio shape in validation : {example_val_batch.shape}')
            print(f'Label shape in validation : {label.shape}')

        for example_test_batch, label in test.take(1):
            print(f'Audio shape in test : {example_test_batch.shape}')
            print(f'Label shape in test : {label.shape}')
    if label_names is not None:
        return INPUT_DIM, len(label_names)
    else:
        return INPUT_DIM


def K_fold_training(dataset, build_model, 
                    params = {},
                    K = 5,
                    epochs=2, 
                    verbose = 2,
                    patience = 10,
                    **kwargs):
        
    callbacks_loss = [tf.keras.callbacks.EarlyStopping(monitor='loss', 
                                                      mode='auto', 
                                                      verbose=verbose,
                                                      patience=patience)]
    callback_acc = [tf.keras.callbacks.EarlyStopping(monitor='accuracy',
                                                    mode='auto',
                                                    verbose=verbose,
                                                    patience=patience)]

    #build the keras-sklearn regressor
    #temp_params = {key_param:params[key_params][0] for key_param in params.keys()}
    model = KerasClassifier( model = build_model, epochs = epochs, verbose = verbose, callbacks = [callbacks_loss,callback_acc], **params)
    
    def my_custom_loss_func(y_true, y_pred, verbose = verbose):
        y_pred = np.argmax(y_pred, axis=1)
        y_true = np.argmax(y_true, axis=1)
        if verbose > 0:
            print(f'accuracy on test for this fold is {accuracy_score(y_true, y_pred)}')
        return accuracy_score(y_true, y_pred)
    score = {'one_hot_accuracy':make_scorer(my_custom_loss_func, greater_is_better=True)}

    grid_search = GridSearchCV(estimator = model, param_grid = params, cv=K, verbose=verbose, scoring = score, refit=False)

    #extract X and y from the dataset
    X = np.concatenate([x for x, y in dataset], axis =0)
    y = np.concatenate([y for x, y in dataset], axis=0)


    #train a k-fold cross validation
    model_cv = grid_search.fit(X, y)
    result = pd.DataFrame( pd.DataFrame(model_cv.cv_results_, index = model_cv.cv_results_['params']).mean_test_one_hot_accuracy)
    result.columns = ['mean_accuracy']
    best_params = result.index[np.argmax(result.mean_accuracy)]

    if verbose > 0:
        print(f'The best parameters are {best_params}')
        print(f'The accuracy score are')
    
    display(result)

    return model_cv, result, best_params


def compile_and_fit(model, train_data, val_data, 
                    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), 
                    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-3) if sys.platform == 'darwin' else tf.keras.optimizers.Adam(learning_rate=1e-3), 
                    metrics = ['accuracy'],
                    patience = 5,
                    epochs = 10,
                    steps_per_epoch = None,
                    verbose = 1,
                    model_filename = None):
    
    if verbose > 0:
        model.summary()
    
    model.compile(optimizer=optimizer,
                  loss=loss,
                  metrics=metrics)
    
    if model_filename is not None:
        callbacks = [tf.keras.callbacks.EarlyStopping(verbose=verbose, patience=patience),
                     ModelSaveCallback(model_filename)]
        #cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1)

    else:
        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', 
                                                      mode='max', 
                                                      verbose=verbose,
                                                      restore_best_weights=True, 
                                                      patience=patience)]
    
    history = model.fit(train_data,
                        epochs=epochs,
                        validation_data=val_data,
                        callbacks=callbacks,
                        steps_per_epoch = steps_per_epoch,
                        verbose=verbose)
    return model,history


@tf.autograph.experimental.do_not_convert
def compile_fit_evaluate(data_frame, model, train, val, test, label_names=None,
                         load_model = False,
                         model_path = None,
                         loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), 
                         optimizer =tf.keras.optimizers.legacy.Adam(learning_rate=1e-3) if sys.platform == 'darwin' else tf.keras.optimizers.Adam(learning_rate=1e-3), 
                         metrics = ['accuracy'],
                         patience = 5,
                         epochs = 10,
                         steps_per_epoch = None,
                         verbose = 1,
                         show_history = True,
                         show_test_evaluation = True,
                         show_confusion_matrix = True,
                         listen_to_wrong = True,
                         save_the_model = False,
                         name_model = None,
                         save_weights_only = False,
                         save_format = 'tf',
                         save_best_only = True,
                         model_filename = None,
                         ):
    if load_model:
        if model_path is None:
            print("You need to specify the path to load the model")
        else:
            model = tf.keras.models.load_model(model_path)

    model,history = compile_and_fit(model, train, val,
                                    optimizer=optimizer,
                                    loss=loss,  
                                    metrics=metrics,
                                    patience=patience,
                                    epochs=epochs,
                                    steps_per_epoch=steps_per_epoch,
                                    verbose=verbose,
                                    model_filename=model_filename)

    if show_history:
        plot_history(history)

    if show_test_evaluation:
        scores = model.evaluate(test, return_dict=True)
        display(scores)

    if show_confusion_matrix:
        #predict the test set
        y_pred = model.predict(test)
        y_pred = tf.argmax(y_pred, axis=1)

        y_true = tf.concat(list(test.map(lambda s,lab: lab)), axis=0)
        y_true = tf.argmax(y_true,axis=1)

        #confusion matrix
        confusion_mtx = confusion_matrix(y_true, y_pred, label_names)

    if listen_to_wrong:
        listen_to_wrong_audio(data_frame, y_true, y_pred, label_names, confusion_mtx)

    if save_the_model:
        if name_model is None:
            print("You need to specify the path to save the model")
        else:
            path_to_save = os.path.join(main_dir,'models',name_model)
            if save_weights_only:
                model.save_weights(path_to_save, save_format=save_format)
            else:
                model.save(path_to_save, save_format=save_format, save_best_only=save_best_only)

    if show_confusion_matrix:
        if show_test_evaluation:
            return model, history, confusion_mtx, scores
        
    else:
        return model, history
    

