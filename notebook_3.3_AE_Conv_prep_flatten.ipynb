{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YZ9hEhaKv5D"
      },
      "source": [
        "### Cella usa e getta (ogni volta che re-inizializzi il kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_D9MJ9FKv5E",
        "outputId": "a30d30ac-23e6-46d4-bd95-e8df038d32e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Installing tensorflow-IO\n",
            "Requirement already satisfied: tensorflow-io in /usr/local/lib/python3.10/dist-packages (0.33.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.33.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-io) (0.33.0)\n",
            "Installing keras\n",
            "Collecting keras==2.10.0\n",
            "  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires keras<2.13,>=2.12.0, but you have keras 2.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.10.0\n",
            "Installing scikeras\n",
            "Requirement already satisfied: scikeras[tensorflow] in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras[tensorflow]) (23.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras[tensorflow]) (1.2.2)\n",
            "Requirement already satisfied: tensorflow>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from scikeras[tensorflow]) (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras[tensorflow]) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (0.4.14)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow>=2.11.0->scikeras[tensorflow])\n",
            "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.11.0->scikeras[tensorflow]) (0.33.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.11.0->scikeras[tensorflow]) (0.41.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.11.0->scikeras[tensorflow]) (0.2.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (2.3.7)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.11.0->scikeras[tensorflow]) (3.2.2)\n",
            "Installing collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.10.0\n",
            "    Uninstalling keras-2.10.0:\n",
            "      Successfully uninstalled keras-2.10.0\n",
            "Successfully installed keras-2.12.0\n",
            "installing keras tuner\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.7.22)\n",
            "Requirement already satisfied: numba==0.57.0 in /usr/local/lib/python3.10/dist-packages (0.57.0)\n",
            "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba==0.57.0) (0.40.1)\n",
            "Requirement already satisfied: numpy<1.25,>=1.21 in /usr/local/lib/python3.10/dist-packages (from numba==0.57.0) (1.23.5)\n",
            "Adding the folder for the modules\n",
            "TensorFlow version: 2.12.0\n",
            "keras version = 2.12.0\n"
          ]
        }
      ],
      "source": [
        "# libraries\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "print(sys.version)\n",
        "\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "strong_pc = platform.system() == 'Linux'\n",
        "\n",
        "if in_colab:\n",
        "    if not os.getcwd().split('/')[-1].split('_')[-1]=='2023':\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        os.chdir(r'/content/drive/MyDrive/Human_Data_Analytics_Project_2023')\n",
        "\n",
        "    if not 'tensorflow_io' in sys.modules:\n",
        "        print('Installing tensorflow-IO')\n",
        "        !pip install tensorflow-io\n",
        "    if not 'keras' in sys.modules:\n",
        "        print('Installing keras')\n",
        "        !pip install keras==2.10.0\n",
        "    if not 'scikeras' in sys.modules:\n",
        "        print('Installing scikeras')\n",
        "        !pip install scikeras[tensorflow]\n",
        "    if not 'keras-tuner' in sys.modules:\n",
        "        print('installing keras tuner')\n",
        "        !pip install keras-tuner\n",
        "        !pip install numba==0.57.0\n",
        "\n",
        "main_dir = os.getcwd()\n",
        "if main_dir not in sys.path:\n",
        "    print('Adding the folder for the modules')\n",
        "    sys.path.append(main_dir)\n",
        "\n",
        "#BASE LIBRARIES\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import shutil\n",
        "import time\n",
        "import random\n",
        "import subprocess\n",
        "import itertools\n",
        "import warnings\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "#PLOT LIBRARIES\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sb\n",
        "sb.set(style=\"white\", palette=\"muted\")\n",
        "import IPython.display as ipd\n",
        "#import plotly.express as px\n",
        "\n",
        "#AUDIO LIBRARIES\n",
        "import librosa\n",
        "from scipy.io import wavfile\n",
        "from scipy import signal\n",
        "from scipy.fft import fft,ifft,fftfreq, fftshift\n",
        "from scipy.signal import stft,spectrogram,periodogram\n",
        "#from pydub import AudioSegment\n",
        "\n",
        "#MACHINE LEARNING LIBRARIES\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, LeaveOneOut, train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.utils import check_random_state\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.utils import plot_model as tf_plot\n",
        "if in_colab:\n",
        "    import tensorflow_io as tfio\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "# show keras version\n",
        "import keras\n",
        "print(f'keras version = {keras.__version__}')\n",
        "#import keras_tune as kt\n",
        "from keras import layers\n",
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.regularizers import L1L2\n",
        "# kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4) # we may use this in some layers...\n",
        "\n",
        "#RANDOM SETTINGS\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "check_random_state(seed)\n",
        "\n",
        "#EVALUATION LIBRAIRES\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import RocCurveDisplay, precision_recall_curve, PrecisionRecallDisplay\n",
        "from sklearn.metrics import precision_recall_fscore_support, auc\n",
        "\n",
        "#OUR PERSONAL FUNCTIONS\n",
        "import importlib\n",
        "from Preprocessing.data_loader import download_dataset,load_metadata\n",
        "from Preprocessing.exploration_plots import one_random_audio, plot_clip_overview, Spectral_Analysis\n",
        "from Models.basic_ml import basic_ML_experiments, basic_ML_experiments_gridsearch, build_dataset, extract_flatten_MFCC\n",
        "from Visualization.model_plot import confusion_matrix,listen_to_wrong_audio\n",
        "\n",
        "importlib.reload(importlib.import_module('Preprocessing.data_loader'))\n",
        "importlib.reload(importlib.import_module('Models.basic_ml'))\n",
        "importlib.reload(importlib.import_module('Visualization.model_plot'))\n",
        "\n",
        "from Preprocessing.data_loader import load_metadata\n",
        "#df_ESC10, df_ESC50 = load_metadata(main_dir,heads = False, ESC_US = False, statistics=False)\n",
        "\n",
        "from Preprocessing.data_loader import load_metadata\n",
        "from Models.basic_ml import basic_ML_experiments, basic_ML_experiments_gridsearch, build_dataset, extract_flatten_MFCC\n",
        "\n",
        "importlib.reload(importlib.import_module('Models.ann_utils'))\n",
        "importlib.reload(importlib.import_module('Visualization.model_plot'))\n",
        "\n",
        "from Models.ann_utils import *\n",
        "from Models.ann_utils import MFCCWithDeltaLayer,OutputCutterLayer\n",
        "from Visualization.model_plot import plot_history, confusion_matrix, listen_to_wrong_audio, visualize_the_weights\n",
        "\n",
        "ESC10_path = os.path.join(main_dir,'Data', 'ESC-10-depth')\n",
        "samplerate = 44100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RlsGQOx3q-9"
      },
      "source": [
        "# 3 UNSUPERVISED LEARNING: AUTOENCODERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z4wHcjFm3q--"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "importlib.reload(importlib.import_module('Models.ann_utils'))\n",
        "importlib.reload(importlib.import_module('Visualization.model_plot'))\n",
        "importlib.reload(importlib.import_module('Preprocessing.data_loader'))\n",
        "from Models.ann_utils import *\n",
        "from Preprocessing.data_loader import reshape_US\n",
        "from Visualization.model_plot import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOuuLDKGHtdI"
      },
      "source": [
        "## 3.3 Autoencoder on preprocessed audio - Convolutional and flatten code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHSN_RMjpzc7"
      },
      "source": [
        "As we have seen before the MEL preprocessing is always beaten by one of the other two. For this reason we are going to avoid this type of computations and we will train the autoencoder to reconstruct the MFCC or the STFT only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKT-FEoFHtdI"
      },
      "source": [
        "### Create the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PSHjSRlHtdI",
        "outputId": "795c6d12-8212-4c1c-fc8b-681eacec02a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating the dataset from folder 01\n"
          ]
        }
      ],
      "source": [
        "preprocessing = 'STFT'\n",
        "AE_name = 'AE_Conv_prep_flatten_'+preprocessing\n",
        "train, val, test, INPUT_DIM = create_US_dataset(\n",
        "        preprocessing=preprocessing,\n",
        "        folder_number=1,\n",
        "        main_dir = main_dir,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAnv_UG63q_J",
        "outputId": "6b8abf7b-9a32-46f4-f60e-a7fbdd9bbad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created AE_Conv_prep_flatten_STFT_count.txt with content '0' in folder Saved_Models\n"
          ]
        }
      ],
      "source": [
        "folder_path = 'Saved_Models'  # Replace this with the actual folder path\n",
        "file_names = [ AE_name+'_count.txt']\n",
        "\n",
        "for name in file_names:\n",
        "    file_path = os.path.join(main_dir, folder_path, name)\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write('0')\n",
        "    print(f\"Created {name} with content '0' in folder {folder_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mH5ixyZHtdJ"
      },
      "source": [
        "To interpret the mean squared error the we are going to optimize in our training we need to compute some mse between our images and some random images or between couple of images in our training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6_u-ghOHtdJ",
        "outputId": "c39fad94-4f34-4dbb-e104-2170656241e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean MSE between an audio preprocessed and a random image (tf) : 0.97840\n",
            "Mean MSE between an audio preprocessed and a random image (np): 0.97995\n",
            "Mean MSE between two random images (tf-np): 0.66618\n"
          ]
        }
      ],
      "source": [
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Extract the first batch from the training set: x is a tuple of arrays of dim (128,220500)\n",
        "for x, y in train.take(1):\n",
        "    #compute the mse between the first batch and a batch of random images with the same shape\n",
        "    random_images_1 = tf.random.uniform(shape=x.shape, minval = -1, maxval = 1,dtype=tf.float32)\n",
        "    random_images_2 = np.random.uniform(-1, 1, size = x.shape)\n",
        "    print(f'Mean MSE between an audio preprocessed and a random image (tf) : {mse(x, random_images_1):.5f}')\n",
        "    print(f'Mean MSE between an audio preprocessed and a random image (np): {mse(x, random_images_2):.5f}')\n",
        "    print(f'Mean MSE between two random images (tf-np): {mse(random_images_1, random_images_2):.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Df6eFEmHtdJ"
      },
      "source": [
        "### Preparation to use Keras-Tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksdQ9JsJHtdJ"
      },
      "source": [
        "Now we define a function to build a generic convolutional autoencoder. We'll give this function to a keras tuner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d-_skiUOHtdK"
      },
      "outputs": [],
      "source": [
        "#General function to build an autoencoder\n",
        "#CONVOLUTIONAL AUTOENCODER WITH VECTORIAL CODE\n",
        "code_size = 32\n",
        "n_layers = 2\n",
        "n_units = 32\n",
        "\n",
        "# the real build function for general autoencoder (keras code)\n",
        "def build_autoencoder(img_shape = INPUT_DIM,\n",
        "                      code_size = code_size,\n",
        "                      activation = 'tanh',\n",
        "                      padding = 'valid',\n",
        "                      n_layers = n_layers, #max number of layers is 3\n",
        "                      n_units = n_units,\n",
        "                      kernel_size = (3,3),\n",
        "                      strides = (2,2),\n",
        "                      max_pooling = (2,2),\n",
        "                      regularizer = 1e-4,\n",
        "                      batch_norm = True,\n",
        "                      drop_out = 0.0,\n",
        "                      learning_rate = 1e-3,\n",
        "                      loss = tf.keras.losses.MeanSquaredError(),\n",
        "                      metrics = ['mse'],\n",
        "                      AE_name = AE_name\n",
        " ):\n",
        "    lr = learning_rate\n",
        "    # encoder\n",
        "    encoder = tf.keras.Sequential(name='Encoder')\n",
        "    encoder.add(tf.keras.Input(img_shape))\n",
        "    for i in range(n_layers):\n",
        "        encoder.add(layers.Conv2D(n_units * (i+1), kernel_size,strides = strides, activation = activation, padding=padding))\n",
        "        encoder.add(layers.MaxPool2D(max_pooling, padding='same'))\n",
        "        if batch_norm:\n",
        "            encoder.add(layers.BatchNormalization())\n",
        "        if drop_out > 0:\n",
        "            encoder.add(layers.Dropout(drop_out))\n",
        "\n",
        "    # flatten layer to get the code\n",
        "    my_shape = encoder.layers[-1].output_shape\n",
        "    encoder.add(layers.Flatten())\n",
        "    encoder.add(layers.Dense(code_size,activation = activation, activity_regularizer=keras.regularizers.l1(regularizer)))\n",
        "\n",
        "    # decoder\n",
        "    decoder = tf.keras.Sequential(name='Decoder')\n",
        "    decoder.add(tf.keras.Input(code_size))\n",
        "    decoder.add(layers.Dense(np.prod(my_shape[1:]), activation=activation))\n",
        "    decoder.add(layers.Reshape(my_shape[1:]))\n",
        "\n",
        "    # transpose convolutions\n",
        "    for i in range(n_layers):\n",
        "        filters = n_units * (n_layers-i) if i<n_layers-1 else 1\n",
        "        decoder.add(layers.Conv2DTranspose(filters , kernel_size, strides=strides, activation=activation, padding=padding))\n",
        "        decoder.add(layers.UpSampling2D(size=max_pooling))\n",
        "        if batch_norm:\n",
        "            decoder.add(layers.BatchNormalization())\n",
        "\n",
        "    #final reshape\n",
        "    decoder.add(tf.keras.layers.Resizing(height = INPUT_DIM[0], width = INPUT_DIM[1], interpolation=\"bilinear\", crop_to_aspect_ratio=False))\n",
        "\n",
        "    # build the autoencoder with keras.Model\n",
        "    inp = tf.keras.Input(shape = INPUT_DIM)\n",
        "    code = encoder(inp)\n",
        "    reconstruction = decoder(code)\n",
        "    autoencoder = tf.keras.Model(inputs=inp, outputs=reconstruction, name = AE_name)\n",
        "\n",
        "    # compile the autoencoder\n",
        "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr) if sys.platform == 'darwin' or in_colab else tf.keras.optimizers.Adam(learning_rate = lr)\n",
        "    loss = loss\n",
        "    metrics = metrics\n",
        "\n",
        "    autoencoder.compile(optimizer=optimizer,\n",
        "                loss=loss,\n",
        "                metrics=metrics)\n",
        "\n",
        "    #print the number of trainable parameters\n",
        "    print(f'Model built with { sum(tf.keras.backend.count_params(p) for p in autoencoder.trainable_variables)} trainable params')\n",
        "\n",
        "    return autoencoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_0cGwG6HtdK",
        "outputId": "5872839f-1fff-4cbc-f007-477620f06b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model built with 226403 trainable params\n"
          ]
        }
      ],
      "source": [
        "verbose = 0\n",
        "#test the build_autoencoder function\n",
        "autoencoder = build_autoencoder(n_layers = 3)\n",
        "if verbose > 1:\n",
        "    autoencoder.summary(line_length=100)\n",
        "    autoencoder.layers[1].summary(line_length=100)\n",
        "    autoencoder.layers[2].summary(line_length=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4V-LCC3LHtdL"
      },
      "outputs": [],
      "source": [
        "# function to build the model using different hyperparameters (keras tuner code)\n",
        "def build_model(hp,test=False):\n",
        "\n",
        "    #define hyperparameters\n",
        "    if test: #if test is true you run the tuner only on a reduced hyperparameter space\n",
        "        print('Running a test smaller grid search')\n",
        "        n_units = 32\n",
        "        n_layers = hp.Choice(name = 'n_layers', values =  [2,3])\n",
        "        kernel_size = 3\n",
        "        strides = 2\n",
        "        max_pooling = 2\n",
        "        regularizer = hp.Choice(name = 'regularizer', values = [1e-4,0.0])\n",
        "        padding = 'same'\n",
        "        code_size = 32\n",
        "        activation = 'tanh'\n",
        "        drop_out = hp.Choice(name = 'drop_out', values = [0.25,0.0])\n",
        "        batch_norm = True\n",
        "        lr_max, lr_min = 1e-3, 1e-3\n",
        "        hp_lr = hp.Float('learning_rate', min_value=lr_min, max_value=lr_max, sampling='log')\n",
        "    else:\n",
        "        n_units = hp.Choice(name = 'n_units', values =  [4,8,16,32,64,128], default = 32)\n",
        "        n_layers = hp.Int(name = 'n_layers', min_value = 1, max_value = 3, step=1, sampling='linear', default = 2)\n",
        "        kernel_size = hp.Choice(name = 'kernel_size', values = [3,5,7], default = 3)\n",
        "        strides = hp.Choice(name = 'strides', values = [2,3], default = 2)\n",
        "        max_pooling = hp.Choice(name = 'max_pooling' , values = [2,3], default = 2)\n",
        "        regularizer = hp.Choice(name = 'regularizer', values = [0.0,1e-2,1e-3,1e-4,1e-5], default = 1e-4)\n",
        "        padding = hp.Choice(name = 'padding', values = ['same','valid'], default = 'valid')\n",
        "        code_size = hp.Choice(name = 'code_size', values = [32,64,128], default = 32)\n",
        "        activation = hp.Choice(name = 'activation', values = ['relu','elu','tanh'], default = 'tanh')\n",
        "        drop_out = hp.Choice(name = 'drop_out', values = [0.0, 0.25, 0.5], default = 0.0)\n",
        "        batch_norm = hp.Choice(name = 'batch_norm', values = [True, False], default = True)\n",
        "        lr_min, lr_max =1e-4, 1e-1\n",
        "        learning_rate= hp.Choice('learning_rate', values = [1e-4, 1e-3, 5*1e-3, 1e-2,5*1e-2,1e-1], default = 1e-3)\n",
        "\n",
        "\n",
        "    model = build_autoencoder(code_size = code_size,\n",
        "                              activation = activation,\n",
        "                              padding = padding,\n",
        "                              n_layers = n_layers,\n",
        "                              n_units = n_units,\n",
        "                              kernel_size = (kernel_size,kernel_size),\n",
        "                              strides = (strides,strides),\n",
        "                              max_pooling = (max_pooling, max_pooling),\n",
        "                              regularizer = regularizer,\n",
        "                              batch_norm = batch_norm,\n",
        "                              drop_out = drop_out,\n",
        "                              learning_rate= learning_rate)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTSyPl53HtdL",
        "outputId": "465a9704-6532-4004-dcab-f9b4744c418f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model built with 189795 trainable params\n",
            "Model: \"AE_Conv_prep_flatten_STFT\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 64, 128, 1)]      0         \n",
            "                                                                 \n",
            " Encoder (Sequential)        (None, 32)                84768     \n",
            "                                                                 \n",
            " Decoder (Sequential)        (None, 64, 128, 1)        105349    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 190,117\n",
            "Trainable params: 189,795\n",
            "Non-trainable params: 322\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#test the build_model function\n",
        "build_model(kt.HyperParameters()).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFNuQ9P7HtdM"
      },
      "source": [
        "### Implement the grid search hyperparamter-wise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2L10vGSHtdM",
        "outputId": "3824ffe4-9c6e-46b3-bd3a-55a0d7324ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for the best value for strides\n",
            "Using Random Search strategy for HPO\n",
            "Model built with 189795 trainable params\n",
            "Model built with 89955 trainable params\n",
            "Model built with 189795 trainable params\n",
            "Model built with 89955 trainable params\n",
            "2/2 [==============================] - 1s 31ms/step - loss: 0.0355 - mse: 0.0355\n",
            "The best value for strides is 3, the best score is 0.03549667447805405\n",
            "Searching for the best value for activation\n",
            "Using Random Search strategy for HPO\n",
            "Model built with 89955 trainable params\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model built with 89955 trainable params\n",
            "Model built with 89955 trainable params\n",
            "Model built with 89955 trainable params\n",
            "Model built with 89955 trainable params\n",
            "2/2 [==============================] - 1s 27ms/step - loss: 0.0403 - mse: 0.0403\n",
            "The best value for activation is tanh, the best score is 0.04025600478053093\n",
            "Searching for the best value for n_units\n",
            "Using Random Search strategy for HPO\n",
            "Model built with 89955 trainable params\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model built with 89955 trainable params\n",
            "Model built with 31171 trainable params\n"
          ]
        }
      ],
      "source": [
        "# dictionary with the default values of the hyperparams to be update each time\n",
        "default_values = {\n",
        "    'n_layers': 2,\n",
        "    'n_units': 32,\n",
        "    'kernel_size': 3,\n",
        "    'strides':2,\n",
        "    'max_pooling':2,\n",
        "    'regularizer':0.0,\n",
        "    'padding':'valid',\n",
        "    'code_size':32,\n",
        "    'activation':'tanh',\n",
        "    'drop_out':0.0,\n",
        "    'batch_norm':True,\n",
        "    'learning_rate':1e-3\n",
        "}\n",
        "\n",
        "key_list = list(default_values.keys())\n",
        "\n",
        "#define the general variables for our tuner\n",
        "hpo_methods = ['RandomSearch', 'BayesianOptimization','Hyperband']\n",
        "problematic_combination = []\n",
        "max_model_size = 10**6\n",
        "max_trials = 10\n",
        "dir_name = AE_name\n",
        "verbose=0\n",
        "\n",
        "if strong_pc:\n",
        "    train_small = train\n",
        "    val_small = val\n",
        "else:\n",
        "    small_size_dataset = 40\n",
        "    train_val_small = train.unbatch().take(small_size_dataset)\n",
        "    train_small = train_val_small.skip(10).batch(25)\n",
        "    val_small = train_val_small.take(10).batch(25)\n",
        "\n",
        "# define a list to collect all the best scores\n",
        "best_score_dict ={\n",
        "    'RandomSearch': [],\n",
        "    'BayesianOptimization': [],\n",
        "    'Hyperband': []\n",
        "}\n",
        "\n",
        "#to be consistent with this type of grd search we should pass each hp more than one time...\n",
        "for hpo_method in hpo_methods:\n",
        "    random.shuffle(key_list)\n",
        "    for hyper_params in key_list:\n",
        "        print(f'Searching for the best value for {hyper_params}')\n",
        "\n",
        "        #define an hp set with all fix but one\n",
        "        hp = kt.HyperParameters()\n",
        "\n",
        "        for fixed_param in default_values.keys():\n",
        "            if fixed_param != hyper_params:\n",
        "                hp.Fixed(name = fixed_param, value = default_values[fixed_param])\n",
        "\n",
        "        if verbose>1:\n",
        "            display(hp.space)\n",
        "\n",
        "        try:\n",
        "            #create a tuner for the params not fixed\n",
        "            tuner = build_tuner(build_model = build_model,\n",
        "                                hpo_method = hpo_method,\n",
        "                                max_model_size = max_model_size,\n",
        "                                max_trials = max_trials,\n",
        "                                dir_name = dir_name,\n",
        "                                overwrite = True,\n",
        "                                objective = kt.Objective('val_mse', direction='min'),\n",
        "                                hp=hp,\n",
        "                                not_fixed_param = hyper_params,\n",
        "                                tune_new_entries=True\n",
        "                                )\n",
        "\n",
        "            if verbose>2:\n",
        "                display(tuner.search_space_summary(extended = True))\n",
        "\n",
        "            #fit the tuner\n",
        "            epochs = 50\n",
        "            patience = 10\n",
        "            metrics = ['mse']\n",
        "            callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_'+metrics[0],\n",
        "                                                        verbose=verbose,\n",
        "                                                        patience=patience)]\n",
        "\n",
        "            tuner.search(train_small, validation_data = val_small,\n",
        "                                callbacks=callbacks,\n",
        "                                epochs=epochs,\n",
        "                                verbose=int(verbose>0),\n",
        "                        )\n",
        "\n",
        "            #retrive the best value for the free hp\n",
        "            best_value = tuner.get_best_hyperparameters()[0].values[hyper_params]\n",
        "\n",
        "            #retrive the best score reached\n",
        "            best_score = tuner.get_best_models(num_models=1)[0].evaluate(val, return_dict = True)['mse']\n",
        "\n",
        "            print(f'The best value for {hyper_params} is {best_value}, the best score is {best_score}')\n",
        "            best_score_dict[hpo_method].append(best_score)\n",
        "\n",
        "            #update the default dict of values\n",
        "            default_values[hyper_params] = best_value\n",
        "\n",
        "            #save the updated dictionary\n",
        "            file_path = os.path.join(main_dir, dir_name, hpo_method+'_best_params')\n",
        "            with open(file_path, 'wb') as file:\n",
        "                pickle.dump(default_values, file)\n",
        "\n",
        "            #delete the folder just created by the run\n",
        "            shutil.rmtree(os.path.join(main_dir,dir_name, hpo_method+'_'+hyper_params))\n",
        "\n",
        "        except:\n",
        "            problematic_combination.append(('search_for'+hyper_params, default_values))\n",
        "\n",
        "    with open(file_path, 'rb') as file:\n",
        "        best_params = pickle.load(file)\n",
        "\n",
        "    display(best_params)\n",
        "\n",
        "#save the best_score_dict\n",
        "file_path = os.path.join(main_dir, dir_name, 'best_scores'+preprocessing)\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(best_score_dict, file)\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    best_scores = pickle.load(file)\n",
        "\n",
        "display(best_scores)\n",
        "\n",
        "display(problematic_combination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALwBqeqEHtdM"
      },
      "outputs": [],
      "source": [
        "# compare the best hp from the 3 grid search methods\n",
        "hyperparamters = []\n",
        "for hpo_method in  ['RandomSearch', 'BayesianOptimization','Hyperband']:\n",
        "    file_path = os.path.join(main_dir, dir_name, hpo_method+'_best_params')\n",
        "    with open(file_path, 'rb') as file:\n",
        "        hyperparamters.append(pickle.load(file))\n",
        "pd.DataFrame(hyperparamters, index = ['RandomSearch', 'BayesianOptimization','Hyperband'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjmgOIitHtdM"
      },
      "source": [
        "### Train the model with the best params on more data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ciao)"
      ],
      "metadata": {
        "id": "0sYpJfQxtKX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE2lsOOcHtdN"
      },
      "outputs": [],
      "source": [
        "insert_by_hand = True\n",
        "\n",
        "if insert_by_hand:\n",
        "    best_params = {\n",
        "        'n_layers':1,\n",
        "        'n_units':64,\n",
        "        'kernel_size':3,\n",
        "        'strides':3,\n",
        "        'max_pooling':3,\n",
        "        'regularizer':0.0001,\n",
        "        'padding':'valid',\n",
        "        'code_size':128,\n",
        "        'activation':'elu',\n",
        "        'drop_out':0.25,\n",
        "        'batch_norm':False,\n",
        "        'learning_rate':0.00175,\n",
        "    }\n",
        "else:\n",
        "    file_path = os.path.join(main_dir, dir_name, 'Hyperband'+'_best_params')\n",
        "    with open(file_path, 'rb') as file:\n",
        "        best_params = pickle.load(file)\n",
        "\n",
        "\n",
        "# build an autoencoder with the best params\n",
        "autoencoder = build_autoencoder(**best_params)\n",
        "\n",
        "#autoencoder = tuner.get_best_models(num_models=1)[0] #to create the model with some already wuite good weights\n",
        "autoencoder.summary()\n",
        "verbose=0\n",
        "if verbose>0:\n",
        "    autoencoder.layers[1].summary()\n",
        "    autoencoder.layers[2].summary()\n",
        "\n",
        "epochs = 1 if not strong_pc else 100\n",
        "n_folders = 2 if not strong_pc else 50 #then you can restart and train on more folders\n",
        "\n",
        "US_training(AE_name = AE_name, autoencoder = autoencoder, epochs = epochs , n_folders=n_folders, preprocessing = preprocessing, ndim=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2uQtW8ywmpq"
      },
      "source": [
        "### Show the reconstruction capabilities of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hlz2Q-Pxwmpq"
      },
      "outputs": [],
      "source": [
        "#load the saved model\n",
        "model_loaded = tf.keras.models.load_model(os.path.join(main_dir,'Saved_Models',AE_name))\n",
        "model_loaded.summary()\n",
        "\n",
        "#plot the original and reconstructed\n",
        "plot_original_reconstructed(model = model_loaded, n_figures = 5, test=test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there's less variability in the audio from unlabelled dataset we evaluate the autoencoder reconstruction on the labelled dataset."
      ],
      "metadata": {
        "id": "Kaj3F0hOmOnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, val, test, label_names = create_dataset( ESC10_path,\n",
        "                                                verbose = 0,\n",
        "                                                batch_size = 30,\n",
        "                                                validation_split = 0.25, # this is the splitting of train vs validation + test\n",
        "                                                normalize = True, # normalization preprocessing (default is true)\n",
        "                                                preprocessing = preprocessing,   # \"STFT\" or \"MFCC\"\n",
        "                                                show_example_batch = False,\n",
        "                                                ndim=3,\n",
        "                                                resize = True,\n",
        "                                                new_width = 64,\n",
        "                                                new_height= 128)\n",
        "model_loaded = tf.keras.models.load_model(os.path.join(main_dir,'Saved_Models',AE_name))\n",
        "model_loaded.summary()\n",
        "# show n original and reconstructed images\n",
        "plot_original_reconstructed(model = model_loaded, n_figures = 5, test=test)"
      ],
      "metadata": {
        "id": "TpsCbDiKmRND"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}